# Training Runner Configuration
# =============================
# Settings for executing training runs
#
# OPTIMIZED FOR: AMD Ryzen 7 5825U (8 cores, 30GB RAM, no GPU)

# Training mode
# - mock: Generate synthetic losses (for testing pipeline)
# - local: Train locally with PyTorch (CPU)
# - api: Query external training API
mode: mock

# Path settings
paths:
  sweep: outputs/sweep.json
  output: outputs
  checkpoints: outputs/checkpoints
  logs: outputs/logs

# Resume from previous run
resume: true

# ============================================
# Dataset Configuration
# ============================================
dataset:
  # TinyStories - perfect for CPU scaling experiments
  name: roneneldan/TinyStories
  # Alternative small datasets:
  #   - wikitext/wikitext-2-v1 (2M tokens)
  #   - wikitext/wikitext-103-v1 (103M tokens)
  #   - ptb_text_only (1M tokens)
  
  # Tokenizer (GPT-2 works well for TinyStories)
  tokenizer: gpt2
  
  # Sequence length (shorter = faster, 256-512 good for small models)
  seq_len: 256
  
  # Cache tokenized dataset to disk
  cache_dir: .cache/datasets

# ============================================
# CPU Optimization Settings
# ============================================
hardware:
  # Device (cpu or cuda)
  device: cpu
  
  # Number of CPU threads for PyTorch
  # Ryzen 7 5825U: 8 cores / 16 threads
  # Rule: use physical cores for compute, leave some for data loading
  num_threads: 6
  
  # DataLoader workers (parallel data loading)
  num_workers: 4
  
  # Pin memory (useful even on CPU for faster data transfer)
  pin_memory: false
  
  # Mixed precision (bfloat16 can work on CPU but minimal benefit)
  dtype: float32

# ============================================
# Training Hyperparameters
# ============================================
trainer:
  # Batch size (limited by RAM, not VRAM)
  # 30GB RAM can handle batch_size=64 with seq_len=256 for most model sizes
  batch_size: 64
  
  # Gradient accumulation (effective_batch = batch_size * accumulation_steps)
  accumulation_steps: 1
  
  # Learning rate (will be overridden by sweep config)
  learning_rate: 3.0e-4
  
  # LR scheduler
  scheduler: cosine
  warmup_ratio: 0.1  # 10% of training for warmup
  
  # Weight decay
  weight_decay: 0.1
  
  # Gradient clipping
  max_grad_norm: 1.0
  
  # Optimizer
  optimizer: adamw
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_eps: 1.0e-8
  
  # Random seed for reproducibility
  seed: 42
  
  # Logging frequency (steps)
  log_every: 100
  
  # Evaluation frequency (steps)
  eval_every: 500

# ============================================
# Model Architecture Defaults
# ============================================
model:
  # Vocabulary size (GPT-2 tokenizer)
  vocab_size: 50257
  
  # These are overridden by sweep config:
  d_model: 256
  num_layers: 6
  num_heads: 4
  
  # FFN hidden dim (typically 4x d_model)
  ffn_ratio: 4
  
  # Dropout (0 for scaling law experiments)
  dropout: 0.0
  
  # Positional encoding: RoPE (Rotary Position Embedding)
  pos_encoding: rotary

# ============================================
# u-mup (Unit-Scaled Maximal Update Parametrization)
# ============================================
# Ensures hyperparameters transfer across model widths.
# Critical for scaling law experiments!
#
# References:
# - Blake et al. (2024) "u-μP: The Unit-Scaled Maximal Update Parametrization"
# - Yang et al. (2022) "Tensor Programs V"
# - Implementation: Based on VigneshSrinivasan10/scaling-recipes
umup:
  enabled: true
  
  # Initialization
  # std = base_std * (fan_in) ** -0.5
  # Output layer initialized to zero
  base_std: 0.02
  
  # Learning rate scaling
  # lr = base_lr * (base_width / hidden_dim)
  base_width: 32
  
  # Weight decay scaling  
  # wd = base_wd * (hidden_dim / base_width_wd)
  base_width_wd: 1024
  
  # Reduced LR for embeddings/bias/norm (multiplier)
  no_decay_lr_mult: 0.1

# ============================================
# Estimated Training Times (Ryzen 7 5825U)
# ============================================
# These are rough estimates for TinyStories:
#
#   Model Size | 1e12 FLOPs | 1e13 FLOPs | 1e14 FLOPs
#   -----------|------------|------------|------------
#   500K       | ~2 min     | ~20 min    | ~3 hr
#   1M         | ~3 min     | ~30 min    | ~5 hr
#   3M         | ~5 min     | ~50 min    | ~8 hr
#   10M        | ~10 min    | ~1.5 hr    | ~15 hr
#   30M        | ~25 min    | ~4 hr      | ~40 hr
#
# Total sweep (6 budgets × 8 sizes): ~50-100 hrs
# Recommendation: Start with smaller sweep, validate, then expand

# Hydra settings
hydra:
  run:
    dir: .
  output_subdir: null
