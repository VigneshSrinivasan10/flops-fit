---
phase: 05-analysis-and-fitting
plan: "01"
type: tdd
wave: 1
depends_on: []
files_modified:
  - src/flops_fit/analyzer.py
  - tests/test_analyzer.py
autonomous: true

must_haves:
  truths:
    - "fit_power_law() uses linear-space nonlinear least squares (scipy.optimize.least_squares), not log-space linear regression"
    - "PowerLawFit has an l_inf field; fit_power_law() fits and stores the irreducible loss baseline"
    - "Outlier experiments are automatically detected using IQR on residuals and excluded before final fitting"
    - "analyze() applies outlier detection globally before all three power law fits"
    - "R-squared is computed in linear space on inlier points"
  artifacts:
    - path: "src/flops_fit/analyzer.py"
      provides: "Refactored ScalingLawAnalyzer with linear-space fitting and outlier detection"
      contains: "least_squares"
    - path: "tests/test_analyzer.py"
      provides: "TDD tests covering new fitting method and outlier detection"
      exports: ["TestPowerLawFitLinearSpace", "TestOutlierDetection"]
  key_links:
    - from: "fit_power_law()"
      to: "scipy.optimize.least_squares"
      via: "linear-space residual function with L_inf + k * x^a"
      pattern: "least_squares.*residuals"
    - from: "analyze()"
      to: "_detect_outliers_iqr()"
      via: "initial fit then IQR filter then final fit"
      pattern: "_detect_outliers_iqr"
---

<objective>
Refactor the power law fitting in analyzer.py from log-space linear regression to linear-space nonlinear least squares with an irreducible loss term, and add automatic IQR-based outlier detection.

Purpose: The existing fit_power_law() uses log-space regression which biases fits when loss has an additive baseline (irreducible entropy). Linear-space fitting with an explicit L_inf term is the Chinchilla-standard approach. Outlier detection prevents stalled/anomalous training experiments from distorting the fitted power law exponents.

Output: Refactored analyzer.py with linear-space fitting + outlier detection; expanded test suite covering both behaviors via TDD.
</objective>

<execution_context>
@/home/viggie/.claude/get-shit-done/workflows/execute-plan.md
@/home/viggie/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/STATE.md
@.planning/phases/05-analysis-and-fitting/05-RESEARCH.md
@src/flops_fit/analyzer.py
@tests/test_analyzer.py
</context>

<feature>
  <name>Linear-space power law fitting with irreducible loss and IQR outlier detection</name>
  <files>src/flops_fit/analyzer.py, tests/test_analyzer.py</files>
  <behavior>
    PowerLawFit.l_inf field:
    - PowerLawFit(name="L_opt", coefficient_k=k, exponent_a=a, r_squared=r2, l_inf=1.5) -> l_inf=1.5 stored
    - PowerLawFit(name="N_opt", coefficient_k=k, exponent_a=a, r_squared=r2) -> l_inf=None (default)

    PowerLawFit.predict() with l_inf:
    - fit = PowerLawFit(name="L_opt", coefficient_k=2.0, exponent_a=0.5, r_squared=0.99, l_inf=1.0)
    - fit.predict(np.array([4.0])) -> 1.0 + 2.0 * 4.0^0.5 = 1.0 + 4.0 = 5.0
    - fit.predict(np.array([1.0])) -> 1.0 + 2.0 * 1.0 = 3.0

    PowerLawFit.predict() without l_inf (backward compat):
    - fit = PowerLawFit(name="N_opt", coefficient_k=2.0, exponent_a=0.5, r_squared=0.99)
    - fit.predict(np.array([4.0])) -> 2.0 * 4.0^0.5 = 4.0 (unchanged)

    fit_power_law() linear-space fitting:
    - Given x=np.logspace(10, 20, 20), y=1.5 + 0.1*x^0.5 (with ~5% noise)
    - fit_power_law(x, y, "L_opt") -> fit.l_inf approx 1.5, fit.exponent_a approx 0.5, fit.r_squared > 0.95
    - Given x=np.logspace(10, 20, 20), y=0.1*x^0.73 (with ~5% noise)
    - fit_power_law(x, y, "N_opt") -> fit.exponent_a approx 0.73 (abs=0.15), fit.r_squared > 0.9

    fit_power_law() outlier detection:
    - Given 20 clean points + 2 points with y 10x larger than trend
    - fit_power_law(x, y_with_outliers, "test", exclude_outliers=True) ->
        fit.r_squared significantly higher than fitting with all points
        (outliers excluded before final fit)
    - Given fewer than 5 inlier points -> ValueError raised
    - Default: exclude_outliers=True, outlier_iqr_multiplier=1.5

    analyze() outlier detection integration:
    - analyze() calls fit_power_law() with exclude_outliers=True for all three fits (N_opt, D_opt, L_opt)
    - Outliers detected via initial rough fit residuals; excluded experiment indices same for all three fits
    - Logging: "Excluding N outlier(s) (IQR method)" emitted when outliers found
  </behavior>
  <implementation>
    Step 1: Add l_inf: float | None = None field to PowerLawFit dataclass. Update predict() to add l_inf if present. Update to_dict() to include l_inf and update formula string to show "L_inf + k * C^a" when l_inf is set.

    Step 2: Refactor fit_power_law() signature to:
      def fit_power_law(self, x, y, name, exclude_outliers=True, outlier_iqr_multiplier=1.5)

    Implementation uses two-pass fitting:
    - Pass 1 (outlier detection, when exclude_outliers=True and len >= 5):
        Use simple log-space fit just to get rough y_pred for residual computation.
        residuals_vals = y_clean - y_pred_rough
        Compute Q1, Q3, IQR; mask out points outside [Q1-1.5*IQR, Q3+1.5*IQR].
        Log how many excluded. Raise ValueError if fewer than 2 inliers remain.
    - Pass 2 (final fit, always with irreducible loss term):
        Model: y = l_inf + k * x^a, parametrized as [log10(k), a, l_inf]
        Use scipy.optimize.least_squares() with:
          bounds=([-10, -1, 0], [5, 2, np.inf]) — l_inf constrained >= 0
          p0=[log10(mean(y)-min(y)), 0.5, min(y)*0.95] clamped so log10 arg > 0
        Extract log_k, a, l_inf = result.x; k = 10**log_k
        y_pred = l_inf + k * np.power(x_clean, a)
        R² = 1 - SS_res/SS_tot (in linear space, on inlier points only)

    Step 3: Update analyze() to pass exclude_outliers=True to all three fit_power_law() calls. No other changes to analyze() needed; outlier detection is per-fit (not globally pre-filtered).

    Note: The existing test test_fit_power_law() checks exponent approx 0.5 within abs=0.1 and r_squared > 0.9 on noisy data y = 0.1 * x^0.5. The refactored fit should still pass this (the l_inf will converge near 0 for this data). Verify existing tests still pass.
  </implementation>
</feature>

<tasks>

<task type="tdd">
  <name>Task 1: RED - Write failing tests for linear-space fitting and outlier detection</name>
  <files>tests/test_analyzer.py</files>
  <action>
    Add a new test class TestPowerLawFitLinearSpace after the existing TestPowerLawFit class. Add a new test class TestOutlierDetection after that. Do NOT modify existing tests.

    TestPowerLawFitLinearSpace tests to add:

    test_fit_power_law_with_irreducible_loss:
      - analyzer = ScalingLawAnalyzer(results_path=tmp_path/"r.json", output_dir=tmp_path/"a")
      - np.random.seed(0); x = np.logspace(10, 20, 20)
      - y = 1.5 + 0.1 * np.power(x, 0.5) * np.random.uniform(0.95, 1.05, 20)
      - fit = analyzer.fit_power_law(x, y, "L_opt")
      - assert fit.l_inf is not None; assert fit.l_inf == pytest.approx(1.5, abs=0.5)
      - assert fit.exponent_a == pytest.approx(0.5, abs=0.15)
      - assert fit.r_squared > 0.95

    test_fit_power_law_l_inf_stored_in_result:
      - analyzer = ScalingLawAnalyzer(...)
      - np.random.seed(1); x = np.logspace(12, 20, 15); y = 2.0 + 0.05 * np.power(x, 0.4)
      - fit = analyzer.fit_power_law(x, y, "test")
      - assert hasattr(fit, 'l_inf'); assert fit.l_inf is not None; assert fit.l_inf >= 0

    test_power_law_fit_predict_with_l_inf:
      - fit = PowerLawFit(name="L_opt", coefficient_k=2.0, exponent_a=0.5, r_squared=0.99, l_inf=1.0)
      - result = fit.predict(np.array([4.0]))
      - assert result[0] == pytest.approx(5.0)  # 1.0 + 2.0 * sqrt(4) = 5.0

    test_power_law_fit_predict_without_l_inf_backward_compat:
      - fit = PowerLawFit(name="N_opt", coefficient_k=2.0, exponent_a=0.5, r_squared=0.99)
      - result = fit.predict(np.array([4.0]))
      - assert result[0] == pytest.approx(4.0)  # 2.0 * sqrt(4) = 4.0

    test_power_law_fit_to_dict_includes_l_inf:
      - fit = PowerLawFit(name="L_opt", coefficient_k=1.0, exponent_a=-0.1, r_squared=0.95, l_inf=2.5)
      - d = fit.to_dict()
      - assert "l_inf" in d; assert d["l_inf"] == pytest.approx(2.5)

    TestOutlierDetection tests to add:

    test_outlier_detection_excludes_anomalous_points:
      - analyzer = ScalingLawAnalyzer(...)
      - np.random.seed(42); x = np.logspace(10, 20, 20)
      - y_clean = 1.5 + 0.1 * np.power(x, 0.5) * np.random.uniform(0.97, 1.03, 20)
      - y_with_outliers = y_clean.copy()
      - y_with_outliers[5] = y_clean[5] * 10; y_with_outliers[12] = y_clean[12] * 8
      - fit_all = analyzer.fit_power_law(x, y_with_outliers, "test", exclude_outliers=False)
      - fit_clean = analyzer.fit_power_law(x, y_with_outliers, "test", exclude_outliers=True)
      - assert fit_clean.r_squared > fit_all.r_squared  # outlier removal improves fit

    test_outlier_detection_disabled_when_exclude_outliers_false:
      - Same setup as above; fit with exclude_outliers=False should have lower r_squared
      - assert fit_all.r_squared < fit_clean.r_squared  (same assertion from other side)

    test_outlier_detection_skipped_when_fewer_than_5_points:
      - analyzer = ScalingLawAnalyzer(...)
      - x = np.logspace(10, 14, 4); y = 1.5 + 0.1 * np.power(x, 0.5)
      - fit = analyzer.fit_power_law(x, y, "test", exclude_outliers=True)
      - assert fit is not None  # no error raised; just fits without outlier removal

    Run tests: cd /home/viggie/Projects/flops-fit && uv run pytest tests/test_analyzer.py::TestPowerLawFitLinearSpace tests/test_analyzer.py::TestOutlierDetection -x --tb=short 2>&1 | head -50
    These MUST fail (RED). Commit: test(05-01): add failing tests for linear-space fitting and outlier detection
  </action>
  <verify>uv run pytest tests/test_analyzer.py::TestPowerLawFitLinearSpace tests/test_analyzer.py::TestOutlierDetection -x --tb=short 2>&1 | grep -E "(FAILED|ERROR|passed)" | head -20</verify>
  <done>All new tests in TestPowerLawFitLinearSpace and TestOutlierDetection fail with AttributeError or AssertionError (RED state confirmed). Existing tests still pass.</done>
</task>

<task type="tdd">
  <name>Task 2: GREEN - Implement linear-space fitting and outlier detection in analyzer.py</name>
  <files>src/flops_fit/analyzer.py</files>
  <action>
    Modify src/flops_fit/analyzer.py. Import is already present: `from scipy import optimize`. No new imports needed.

    Step 1: Add l_inf field to PowerLawFit dataclass (after r_squared, before k_ci):
      l_inf: float | None = None  # Irreducible loss baseline (L_inf + k*C^a form)

    Step 2: Update PowerLawFit.predict() to use l_inf:
      def predict(self, x: np.ndarray) -> np.ndarray:
          base = self.coefficient_k * np.power(x, self.exponent_a)
          return base + self.l_inf if self.l_inf is not None else base

    Step 3: Update PowerLawFit.to_dict() to include l_inf and update formula:
      Add "l_inf": self.l_inf to returned dict.
      Update formula: if self.l_inf is not None, use f"{self.name} = {self.l_inf:.4f} + {self.coefficient_k:.4e} * C^{self.exponent_a:.4f}"
      else keep existing: f"{self.name} = {self.coefficient_k:.4e} * C^{self.exponent_a:.4f}"

    Step 4: Replace fit_power_law() entirely:

    def fit_power_law(
        self,
        x: np.ndarray,
        y: np.ndarray,
        name: str,
        exclude_outliers: bool = True,
        outlier_iqr_multiplier: float = 1.5,
    ) -> PowerLawFit:
        """
        Fit a power law: y = L_inf + k * x^a (linear space, nonlinear least squares).

        Uses scipy.optimize.least_squares in linear space with an explicit
        irreducible loss term L_inf. Optionally detects and excludes outliers
        using the IQR method on initial fit residuals before the final fit.

        Args:
            x: Independent variable (e.g., compute budget in FLOPs)
            y: Dependent variable (e.g., optimal loss, model size)
            name: Name for the fit (e.g., "N_opt", "L_opt")
            exclude_outliers: If True and len >= 5, detect and exclude outliers
                via IQR method on rough initial fit residuals.
            outlier_iqr_multiplier: IQR multiplier for outlier bounds (standard 1.5)

        Returns:
            PowerLawFit with fitted k, a, r_squared, and l_inf
        """
        # Filter obviously invalid values
        valid = (x > 0) & (y > 0) & np.isfinite(x) & np.isfinite(y)
        x_clean = x[valid]
        y_clean = y[valid]

        if len(x_clean) < 2:
            raise ValueError(f"Not enough valid points to fit {name}")

        # Pass 1: Rough initial fit (log-space, no L_inf) to get residuals for outlier detection
        if exclude_outliers and len(x_clean) >= 5:
            log_x = np.log10(x_clean)
            log_y = np.log10(y_clean)

            def residuals_rough(params):
                return log_y - (params[0] + params[1] * log_x)

            result_rough = optimize.least_squares(
                residuals_rough, [np.mean(log_y), 0.5]
            )
            log_k_r, a_r = result_rough.x
            y_pred_rough = (10 ** log_k_r) * np.power(x_clean, a_r)
            residuals_vals = y_clean - y_pred_rough

            q1 = np.percentile(residuals_vals, 25)
            q3 = np.percentile(residuals_vals, 75)
            iqr = q3 - q1
            lower = q1 - outlier_iqr_multiplier * iqr
            upper = q3 + outlier_iqr_multiplier * iqr

            is_inlier = (residuals_vals >= lower) & (residuals_vals <= upper)
            n_excluded = int((~is_inlier).sum())

            if n_excluded > 0:
                logger.info(f"{name}: Excluding {n_excluded} outlier(s) (IQR method)")
                x_clean = x_clean[is_inlier]
                y_clean = y_clean[is_inlier]

            if len(x_clean) < 2:
                raise ValueError(
                    f"Not enough inlier points after outlier removal for {name}"
                )

        # Pass 2: Final linear-space fit with irreducible loss term: y = L_inf + k * x^a
        # Parametrize as [log10(k), a, l_inf] for scale-invariant optimization
        k_init_arg = max(np.mean(y_clean) - np.min(y_clean), 1e-10)
        p0 = [np.log10(k_init_arg), 0.5, np.min(y_clean) * 0.95]

        def residuals_final(params):
            log_k, a, l_inf = params
            k = 10 ** log_k
            y_pred = l_inf + k * np.power(x_clean, a)
            return y_clean - y_pred

        result = optimize.least_squares(
            residuals_final,
            p0,
            bounds=([-10, -1, 0], [5, 2, np.inf]),
        )
        log_k, a, l_inf = result.x
        k = 10 ** log_k
        y_pred = l_inf + k * np.power(x_clean, a)

        # R² in linear space
        ss_res = np.sum((y_clean - y_pred) ** 2)
        ss_tot = np.sum((y_clean - np.mean(y_clean)) ** 2)
        r_squared = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0

        logger.info(
            f"{name} = {l_inf:.4f} + {k:.4e} * C^{a:.4f} (R² = {r_squared:.4f})"
        )

        return PowerLawFit(
            name=name,
            coefficient_k=k,
            exponent_a=a,
            r_squared=r_squared,
            l_inf=float(l_inf),
        )

    Step 5: Update analyze() — the three fit_power_law() calls are already compatible; they now use the new signature automatically (exclude_outliers=True by default). No changes to analyze() needed beyond ensuring the calls work. Verify the analyze() method still produces valid ScalingAnalysis.

    Run all tests after implementation:
    cd /home/viggie/Projects/flops-fit && uv run pytest tests/test_analyzer.py -x --tb=short 2>&1 | tail -20

    If any existing tests fail due to the refactoring (e.g., test_fit_power_law that checks exponent approx 0.5), adjust tolerance — the new fitting is more accurate. Do NOT weaken the test beyond abs=0.15.

    Commit: feat(05-01): implement linear-space NLS fitting with l_inf and IQR outlier detection
  </action>
  <verify>cd /home/viggie/Projects/flops-fit && uv run pytest tests/test_analyzer.py -x --tb=short 2>&1 | tail -10</verify>
  <done>All tests in tests/test_analyzer.py pass (including existing tests and new TestPowerLawFitLinearSpace + TestOutlierDetection). fit_power_law() uses least_squares in linear space. PowerLawFit.l_inf is populated. Outlier detection reduces distortion on contaminated data.</done>
</task>

</tasks>

<verification>
Run full test suite to check for regressions:
cd /home/viggie/Projects/flops-fit && uv run pytest --tb=short 2>&1 | tail -20

Verify linear-space fitting is in place (grep confirms no log-space regression in fit_power_law):
grep -n "log_y.*idxmin\|log-linear regression\|log space" /home/viggie/Projects/flops-fit/src/flops_fit/analyzer.py || echo "No log-space regression found (correct)"

Verify l_inf is in PowerLawFit:
grep -n "l_inf" /home/viggie/Projects/flops-fit/src/flops_fit/analyzer.py | head -10

Verify outlier detection is present:
grep -n "iqr\|IQR\|outlier\|percentile" /home/viggie/Projects/flops-fit/src/flops_fit/analyzer.py | head -10
</verification>

<success_criteria>
- All existing 144 tests pass (no regressions)
- TestPowerLawFitLinearSpace: 5 tests pass
- TestOutlierDetection: 3 tests pass
- fit_power_law() uses scipy.optimize.least_squares with linear-space residuals
- PowerLawFit.l_inf field exists and is populated by fit_power_law()
- PowerLawFit.predict() adds l_inf when present
- outlier detection fires on contaminated data, improving R-squared vs no-outlier-removal
</success_criteria>

<output>
After completion, create `.planning/phases/05-analysis-and-fitting/05-01-SUMMARY.md`
</output>
