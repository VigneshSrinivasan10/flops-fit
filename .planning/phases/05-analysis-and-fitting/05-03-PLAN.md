---
phase: 05-analysis-and-fitting
plan: 03
type: tdd
wave: 1
depends_on: []
files_modified:
  - tests/test_analyzer.py
  - src/flops_fit/analyzer.py
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "ScalingLawAnalyzer.predict() expected_loss includes l_inf (irreducible loss baseline)"
    - "All 159 existing tests continue to pass after the fix"
  artifacts:
    - path: "tests/test_analyzer.py"
      provides: "Regression test that catches l_inf omission in predict()"
      contains: "test_analyzer_predict_includes_l_inf"
    - path: "src/flops_fit/analyzer.py"
      provides: "Corrected predict() that adds l_fit.get('l_inf', 0) to l_opt"
      contains: "l_fit.get(\"l_inf\""
  key_links:
    - from: "src/flops_fit/analyzer.py"
      to: "l_fit[\"l_inf\"]"
      via: "l_opt += l_fit.get(\"l_inf\") or 0 after power-law calculation"
      pattern: "l_fit\\.get\\(\"l_inf\""
---

<objective>
Fix ScalingLawAnalyzer.predict() to include l_inf in expected_loss when loading from saved JSON.

Purpose: The analyze() + predict() API path silently drops the irreducible loss baseline (l_inf) from expected_loss. The ScalingAnalysis.predict_optimal_size() path correctly applies l_inf via PowerLawFit.predict(), but ScalingLawAnalyzer.predict() manually reconstructs the power law from the JSON dict without adding l_inf. This produces subtly wrong loss predictions for any fit that uses the L_inf + k*C^a form.

Output: One new failing test (RED), one-line fix (GREEN), all 159 tests passing.
</objective>

<execution_context>
@/home/viggie/.claude/get-shit-done/workflows/execute-plan.md
@/home/viggie/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@src/flops_fit/analyzer.py
@tests/test_analyzer.py
</context>

<tasks>

<task type="tdd">
  <name>Task 1: RED - Write failing test for l_inf propagation in ScalingLawAnalyzer.predict()</name>
  <files>tests/test_analyzer.py</files>
  <action>
Add test method `test_analyzer_predict_includes_l_inf` to the `TestScalingLawAnalyzer` class in `tests/test_analyzer.py`.

The test must:
1. Run a minimal end-to-end analyze() + predict() cycle using `tmp_path` (same pattern as `test_predict_returns_optimal_config`)
2. Patch or verify that the saved `scaling_laws.json` has a non-zero `l_inf` in `l_opt_fit` — OR manually write a known `scaling_laws.json` to `tmp_path / "analysis"` with a controlled l_inf value
3. Assert that `analyzer.predict(target_compute)["expected_loss"]` equals the power-law value PLUS l_inf, not just the power-law value alone

Recommended approach (deterministic, no mock dependency):
- Construct a minimal `scaling_laws.json` dict directly and write it to `tmp_path / "analysis" / "scaling_laws.json"`
- Set `l_opt_fit` with known values: `coefficient_k=2.0`, `exponent_a=0.5`, `l_inf=1.5`
- Call `analyzer.predict(4.0)` → expected = 1.5 + 2.0 * 4.0^0.5 = 1.5 + 4.0 = 5.5
- Assert `result["expected_loss"] == pytest.approx(5.5)`

The test MUST FAIL before the fix (current code produces 4.0, not 5.5).

After writing the test, run it to confirm RED:
```
uv run pytest tests/test_analyzer.py::TestScalingLawAnalyzer::test_analyzer_predict_includes_l_inf -v
```
Confirm output contains FAILED.

Commit:
```
git commit -m "test(05-03): add failing test for l_inf propagation in ScalingLawAnalyzer.predict()"
```
  </action>
  <verify>
`uv run pytest tests/test_analyzer.py::TestScalingLawAnalyzer::test_analyzer_predict_includes_l_inf -v` exits non-zero and shows FAILED.
  </verify>
  <done>
Test `test_analyzer_predict_includes_l_inf` exists in TestScalingLawAnalyzer and fails with the current (unfixed) code, confirming the bug is detectable.
  </done>
</task>

<task type="tdd">
  <name>Task 2: GREEN + REFACTOR - Fix ScalingLawAnalyzer.predict() to add l_inf, verify all tests pass</name>
  <files>src/flops_fit/analyzer.py</files>
  <action>
Fix `ScalingLawAnalyzer.predict()` at lines 436-438 in `src/flops_fit/analyzer.py`.

Current code (WRONG — missing l_inf):
```python
n_opt = n_fit["coefficient_k"] * (target_compute ** n_fit["exponent_a"])
d_opt = d_fit["coefficient_k"] * (target_compute ** d_fit["exponent_a"])
l_opt = l_fit["coefficient_k"] * (target_compute ** l_fit["exponent_a"])
```

Fixed code (add l_inf to l_opt only — n_opt and d_opt do not use l_inf):
```python
n_opt = n_fit["coefficient_k"] * (target_compute ** n_fit["exponent_a"])
d_opt = d_fit["coefficient_k"] * (target_compute ** d_fit["exponent_a"])
l_opt = l_fit["coefficient_k"] * (target_compute ** l_fit["exponent_a"])
l_opt += l_fit.get("l_inf") or 0
```

Note: Use `l_fit.get("l_inf") or 0` (not `l_fit.get("l_inf", 0)`) because the JSON stores `"l_inf": null` when l_inf is None — `.get("l_inf", 0)` would return `None` for the null case, while `or 0` correctly coerces None to 0.

After the fix, run the new test to confirm GREEN:
```
uv run pytest tests/test_analyzer.py::TestScalingLawAnalyzer::test_analyzer_predict_includes_l_inf -v
```

Then run the full test suite to confirm no regressions:
```
uv run pytest tests/ -v
```

All tests must pass (159 + 1 new = 160 total).

Commit:
```
git commit -m "fix(05-03): add l_inf to expected_loss in ScalingLawAnalyzer.predict()"
```
  </action>
  <verify>
`uv run pytest tests/ -v` exits 0 with all tests passing, including `test_analyzer_predict_includes_l_inf`.
  </verify>
  <done>
`ScalingLawAnalyzer.predict()["expected_loss"]` equals power-law value plus l_inf for fits with a non-None l_inf. All 160 tests pass (159 original + 1 new regression test).
  </done>
</task>

</tasks>

<verification>
Run the full test suite after both tasks:
```
uv run pytest tests/ -v --tb=short
```
Expected: all tests pass, including `test_analyzer_predict_includes_l_inf`.

Spot-check the fix manually in Python:
```python
import json, pathlib
# Write a known JSON with l_inf, call predict(), verify expected_loss includes l_inf
```
</verification>

<success_criteria>
- `test_analyzer_predict_includes_l_inf` exists in TestScalingLawAnalyzer and passes
- `ScalingLawAnalyzer.predict()` result["expected_loss"] = power_law + l_inf (not just power_law)
- All 159 previously passing tests still pass
- The fix is a single added line: `l_opt += l_fit.get("l_inf") or 0`
</success_criteria>

<output>
After completion, create `.planning/phases/05-analysis-and-fitting/05-03-SUMMARY.md`
</output>
