---
phase: 01-existing-pipeline-baseline
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/conftest.py
  - tests/test_planner.py
  - tests/test_trainer.py
autonomous: true

must_haves:
  truths:
    - "SweepPlanner.save_sweep writes valid JSON to disk with correct schema"
    - "TrainingRunner loads sweep JSON and runs mock training for all experiments"
    - "TrainingRunner resume mode skips completed experiments and preserves their results"
    - "TrainingRunner mock training produces losses in a reasonable range"
  artifacts:
    - path: "tests/conftest.py"
      provides: "Shared fixtures for pipeline test data"
      contains: "sample_sweep_configs"
    - path: "tests/test_planner.py"
      provides: "Expanded planner tests including save_sweep"
      contains: "test_save_sweep"
    - path: "tests/test_trainer.py"
      provides: "Trainer unit tests covering mock train, resume, sweep loading"
      contains: "TestTrainingRunner"
  key_links:
    - from: "tests/conftest.py"
      to: "tests/test_trainer.py"
      via: "shared fixtures"
      pattern: "sample_sweep_configs|sweep_json"
---

<objective>
Create shared test fixtures and write characterization tests for the planner (save_sweep file I/O) and trainer (mock training, resume logic, sweep loading) modules.

Purpose: Lock down planner file output and trainer behavior so refactoring in later phases cannot silently break the plan-train pipeline stages.
Output: conftest.py with shared fixtures, expanded test_planner.py, new test_trainer.py
</objective>

<execution_context>
@/home/viggie/.claude/get-shit-done/workflows/execute-plan.md
@/home/viggie/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-existing-pipeline-baseline/01-RESEARCH.md
@src/flops_fit/planner.py
@src/flops_fit/trainer.py
@tests/test_planner.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create shared test fixtures and expand planner tests</name>
  <files>tests/conftest.py, tests/test_planner.py</files>
  <action>
Create `tests/conftest.py` with shared fixtures:

1. `sample_sweep_configs` fixture: Returns a list of 12 experiment config dicts (3 compute budgets x 4 model sizes) matching `ExperimentConfig.to_dict()` schema. Use exact powers of 10 for compute budgets (1e17, 1e18, 1e19) to avoid bucket rounding issues. Each dict must have keys: experiment_id, compute_budget, model_size, num_tokens, tokens_per_param. Ensure 6*model_size*num_tokens approximately equals compute_budget.

2. `sweep_json` fixture (depends on tmp_path, sample_sweep_configs): Writes sample_sweep_configs to `tmp_path / "sweep.json"` and returns the path.

3. `sample_results` fixture: Returns a list of 12 completed training result dicts matching `TrainingResult.to_dict()` schema. Keys: experiment_id, compute_budget, model_size, num_tokens, final_loss, actual_flops, wall_time_seconds, timestamp, status ("completed"), error_message (None). Use deterministic loss values derived from a simplified scaling law (no randomness).

4. `results_json` fixture (depends on tmp_path, sample_results): Writes sample_results to `tmp_path / "results.json"` and returns the path.

Expand `tests/test_planner.py` with:

5. `test_save_sweep_creates_json_file(tmp_path)`: Create a SweepPlanner with small params (min_flops=1e17, max_flops=1e19, num_compute_budgets=3, num_model_sizes=3), call save_sweep to tmp_path/"sweep.json", assert file exists, load and verify it's valid JSON, verify each entry has the 5 required keys (experiment_id, compute_budget, model_size, num_tokens, tokens_per_param).

6. `test_save_sweep_returns_config_list(tmp_path)`: Same planner as above, call save_sweep, verify return value is a list of dicts with len > 0, verify first entry has all 5 keys.

7. `test_generate_model_sizes_respects_compute_constraint()`: Create planner with min_model_size=10M, max_model_size=10B, call generate_model_sizes with compute_budget=6e16 (max feasible model = 1e16 params which is below max_model_size). Assert all generated model sizes are <= 1e16.

8. `test_sweep_skips_low_token_configs()`: Create planner where some configs would have num_tokens < model_size//10. Verify those are skipped (use small compute budget with large model sizes).
  </action>
  <verify>Run `cd /home/viggie/Projects/flops-fit && uv run pytest tests/conftest.py tests/test_planner.py -v` -- all tests pass.</verify>
  <done>conftest.py has 4 fixtures (sample_sweep_configs, sweep_json, sample_results, results_json). test_planner.py has original 5 tests plus 4 new tests, all passing.</done>
</task>

<task type="auto">
  <name>Task 2: Create trainer characterization tests</name>
  <files>tests/test_trainer.py</files>
  <action>
Create `tests/test_trainer.py` with class `TestTrainingRunner`:

1. `test_load_sweep(sweep_json)`: Create TrainingRunner with sweep_path=sweep_json, output_dir=tmp_path. Call load_sweep(), verify returns list of 12 dicts, each with required keys (experiment_id, compute_budget, model_size, num_tokens).

2. `test_load_sweep_missing_file(tmp_path)`: Create TrainingRunner with nonexistent sweep_path. Call load_sweep(), assert raises FileNotFoundError with message mentioning "sl-plan" (characterizing the stale command name -- add comment noting this references old command name).

3. `test_mock_train_produces_reasonable_loss(tmp_path)`: Seed np.random.seed(42). Create TrainingRunner in mock mode. Call _mock_train(model_size=10_000_000, num_tokens=100_000_000, compute_budget=6e15). Assert loss is between 1.5 and 5.0, actual_flops > 0, wall_time > 0. Add comment: "Characterization: _mock_train uses unseeded np.random internally; we seed externally for determinism."

4. `test_mock_train_loss_scaling(tmp_path)`: Seed np.random.seed(42). Run mock train for small model (1M params, 10M tokens) and large model (100M params, 1B tokens) with same compute budget. Assert large model loss < small model loss (scaling law behavior). Re-seed before each call.

5. `test_run_sweep_completes_all_experiments(tmp_path, sweep_json)`: Seed np.random.seed(42). Create TrainingRunner in mock mode with sweep_path=sweep_json, output_dir=tmp_path. Call run_sweep(resume=False). Assert len(results) == 12 (matching fixture count). Assert all have status="completed". Assert results.json file exists in tmp_path.

6. `test_run_sweep_resume_skips_completed(tmp_path, sweep_json)`: Pre-populate results.json with first 3 experiments completed (use sample_results fixture, take first 3, write to tmp_path/"results.json"). Seed np.random.seed(42). Create TrainingRunner, run_sweep(resume=True). Assert total results == 12. Assert first 3 results have original final_loss values (not overwritten).

7. `test_run_sweep_resume_false_reruns_all(tmp_path, sweep_json)`: Pre-populate results.json with 3 completed results. Seed np.random.seed(42). Run sweep with resume=False. Assert len(results) == 12 (reruns everything, does not load existing).

8. `test_run_experiment_handles_failure(tmp_path)`: Create TrainingRunner with mode="nonexistent" (no _train_fn set). Call run_experiment with valid config dict. Assert result has status="failed" and error_message is not None.

Also add class `TestTrainingResult`:

9. `test_to_dict()`: Create a TrainingResult instance, call to_dict(), verify all 10 keys present.
  </action>
  <verify>Run `cd /home/viggie/Projects/flops-fit && uv run pytest tests/test_trainer.py -v` -- all 9 tests pass.</verify>
  <done>test_trainer.py has 9 tests covering sweep loading, mock training, resume logic, and error handling, all passing.</done>
</task>

</tasks>

<verification>
```bash
cd /home/viggie/Projects/flops-fit && uv run pytest tests/test_planner.py tests/test_trainer.py -v --tb=short
```
All planner and trainer tests pass. No import errors, no fixture resolution failures.
</verification>

<success_criteria>
- conftest.py provides 4 shared fixtures used across test modules
- test_planner.py has 9+ tests including save_sweep file I/O
- test_trainer.py has 9 tests covering mock train, resume, sweep loading, error handling
- All tests pass with `uv run pytest tests/ -v`
- Tests characterize current behavior (including known issues like stale command names in error messages)
</success_criteria>

<output>
After completion, create `.planning/phases/01-existing-pipeline-baseline/01-01-SUMMARY.md`
</output>
