---
phase: 01-existing-pipeline-baseline
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/test_analyzer.py
  - tests/test_model.py
autonomous: true

must_haves:
  truths:
    - "ScalingLawAnalyzer.analyze() produces power law fits with positive R-squared from valid results"
    - "ScalingLawAnalyzer.predict() loads saved analysis and returns optimal config predictions"
    - "ScalingLawAnalyzer.find_optimal_per_budget uses 2-decimal bucket rounding"
    - "GPT model forward pass produces correct output shapes"
    - "GPT model computes loss when labels are provided"
    - "GPT param counting works for both total and non-embedding modes"
    - "create_model_for_scaling produces a model near the target param count"
  artifacts:
    - path: "tests/test_analyzer.py"
      provides: "Expanded analyzer tests covering analyze(), predict(), bucket rounding"
      contains: "test_analyze_produces_valid_fits"
    - path: "tests/test_model.py"
      provides: "GPT model tests covering forward pass, param count, create_model_for_scaling"
      contains: "TestGPT"
  key_links:
    - from: "tests/test_analyzer.py"
      to: "src/flops_fit/analyzer.py"
      via: "ScalingLawAnalyzer class"
      pattern: "ScalingLawAnalyzer"
    - from: "tests/test_model.py"
      to: "src/flops_fit/model.py"
      via: "GPT and GPTConfig classes"
      pattern: "GPT|GPTConfig|create_model_for_scaling"
---

<objective>
Write characterization tests for the analyzer (full analyze flow, predict, bucket rounding) and model (forward pass, param counting, create_model_for_scaling) modules.

Purpose: Lock down analysis pipeline and GPT model behavior. The analyzer tests ensure power law fitting works correctly. The model tests create a safety net for Phase 3 (GPT Plugin Refactor).
Output: Expanded test_analyzer.py, new test_model.py
</objective>

<execution_context>
@/home/viggie/.claude/get-shit-done/workflows/execute-plan.md
@/home/viggie/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-existing-pipeline-baseline/01-RESEARCH.md
@src/flops_fit/analyzer.py
@src/flops_fit/model.py
@tests/test_analyzer.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Expand analyzer tests with full analysis flow</name>
  <files>tests/test_analyzer.py</files>
  <action>
Expand `tests/test_analyzer.py`. Keep existing TestPowerLawFit and TestScalingLawAnalyzer classes. Add new tests:

In TestScalingLawAnalyzer:

1. `test_analyze_produces_valid_fits(tmp_path)`: Generate synthetic pipeline data end-to-end. Create a SweepPlanner (min_flops=1e17, max_flops=1e19, num_compute_budgets=3, num_model_sizes=4), save sweep to tmp_path. Create TrainingRunner in mock mode, seed np.random.seed(42), run_sweep. Then create ScalingLawAnalyzer pointing at the results. Call analyze(). Assert n_opt_fit.r_squared > 0, d_opt_fit.r_squared > 0, l_opt_fit.r_squared > 0. Assert optimal_points is a non-empty list. Assert optimal_ratio is not None and > 0. Assert scaling_laws.json was written to output_dir.

2. `test_predict_returns_optimal_config(tmp_path)`: Run same setup as test above (planner -> trainer -> analyzer.analyze()). Then call analyzer.predict(1e20). Assert result has keys: target_compute, optimal_params, optimal_tokens, expected_loss, tokens_per_param. Assert optimal_params > 0 and optimal_tokens > 0. Assert expected_loss > 0.

3. `test_find_optimal_per_budget_uses_2_decimal_rounding(tmp_path)`: Characterization test. Create analyzer. Build a DataFrame with two experiments that have compute_budgets 1.01e17 and 1.04e17. These should map to different 2-decimal buckets in log10 space (log10(1.01e17)=17.004, log10(1.04e17)=17.017 -- both round to 17.00 at 2 decimals, so they should be in SAME bucket). Assert find_optimal_per_budget returns 1 row. Add comment: "Characterization: analyzer uses np.round(np.log10(budget), 2) for bucketing. Visualizer uses 1-decimal -- known inconsistency."

4. `test_find_optimal_per_budget_selects_min_loss(tmp_path)`: Create DataFrame with 3 experiments at same compute budget, different model sizes and losses. Assert the returned optimal has the minimum loss value.

5. `test_load_results_filters_to_completed(tmp_path)`: Write results JSON with 3 completed and 1 failed experiment. Create analyzer pointing at it. Call load_results(). Assert len == 3 (failed experiment filtered out).

6. `test_fit_power_law_requires_minimum_points(tmp_path)`: Create analyzer. Pass arrays with only 1 valid point (plus some zeros/negatives that get filtered). Assert raises ValueError with "Not enough valid points".

Add new class TestScalingAnalysis:

7. `test_predict_optimal_size()`: Create ScalingAnalysis with known PowerLawFit objects (k=1.0, a=0.5 for all three). Call predict_optimal_size(1e20). Verify result matches manual computation: N_opt = 1.0 * (1e20)^0.5 = 1e10.

8. `test_to_dict()`: Create ScalingAnalysis, call to_dict(). Verify structure has n_opt_fit, d_opt_fit, l_opt_fit, optimal_points, optimal_ratio keys.

Import SweepPlanner and TrainingRunner at top of file for integration-style tests.
  </action>
  <verify>Run `cd /home/viggie/Projects/flops-fit && uv run pytest tests/test_analyzer.py -v` -- all tests pass.</verify>
  <done>test_analyzer.py has 12+ tests covering PowerLawFit, ScalingLawAnalyzer (analyze, predict, bucketing, filtering), and ScalingAnalysis. All pass.</done>
</task>

<task type="auto">
  <name>Task 2: Create GPT model characterization tests</name>
  <files>tests/test_model.py</files>
  <action>
Create `tests/test_model.py` with:

```python
import torch
```

At the top. Import GPT, GPTConfig, create_model_for_scaling, estimate_params_from_config from flops_fit.model.

Class TestGPTConfig:

1. `test_default_d_ff()`: Create GPTConfig with d_model=256. Assert d_ff == 1024 (4 * d_model).

2. `test_custom_d_ff()`: Create GPTConfig with d_model=256, d_ff=512. Assert d_ff == 512.

3. `test_head_divisibility_assertion()`: Create GPTConfig with d_model=256, num_heads=3. Assert raises AssertionError (256 not divisible by 3).

Class TestGPT:

4. `test_forward_output_shape()`: Create GPTConfig(vocab_size=100, d_model=64, num_layers=2, num_heads=2, max_seq_len=32). Create GPT(config). Generate input_ids = torch.randint(0, 100, (2, 16)). Call model(input_ids). Assert logits.shape == (2, 16, 100). Assert loss is None.

5. `test_forward_with_labels_computes_loss()`: Same config. Create input_ids and labels (both random). Call model(input_ids, labels=labels). Assert loss is not None, loss.item() > 0, torch.isfinite(loss).

6. `test_count_parameters_non_embedding()`: Create small GPT. Call count_parameters(non_embedding=True). Assert result > 0 and result < count_parameters(non_embedding=False). The difference should equal vocab_size * d_model (the embedding params).

7. `test_count_parameters_total()`: Create GPT. Call count_parameters(non_embedding=False). Manually compute expected: use estimate_params_from_config with same d_model, num_layers, vocab_size. Assert actual total matches estimate["total"] (they should match since estimate_params_from_config computes exact same formula as the model architecture).

8. `test_umup_initialization_lm_head_is_zero()`: Create GPT with parametrization="u-mup". Assert model.lm_head.weight is all zeros (torch.all(model.lm_head.weight == 0)).

9. `test_umup_initialization_embedding_nonzero()`: Create GPT with parametrization="u-mup". Assert model.token_emb.weight is NOT all zeros.

10. `test_sp_initialization()`: Create GPT with parametrization="sp". Assert model is created without error. Assert lm_head.weight is all zeros (same as u-mup for output layer).

11. `test_from_config_classmethod()`: Call GPT.from_config(d_model=64, num_layers=2, num_heads=2, vocab_size=100, max_seq_len=32). Assert isinstance(model, GPT). Assert model.config.d_model == 64.

Class TestCreateModelForScaling:

12. `test_creates_model_near_target()`: Call create_model_for_scaling(target_params=1_000_000, num_layers=4). Get actual params via model.count_parameters(non_embedding=True). Assert actual is within 2x of target (the approximation is rough). Add comment: "Characterization: uses 12*L*d^2 approximation but actual per-layer params are 16*d^2 (4*d^2 attention + 12*d^2 SwiGLU FFN). Expect actual > target."

13. `test_d_model_is_multiple_of_64()`: Call create_model_for_scaling with various target_params. Assert model.config.d_model % 64 == 0 for each.

14. `test_d_model_divisible_by_num_heads()`: Call create_model_for_scaling. Assert model.config.d_model % model.config.num_heads == 0.

Class TestEstimateParamsFromConfig:

15. `test_estimate_matches_actual_model()`: Create GPTConfig(vocab_size=100, d_model=128, num_layers=4, num_heads=4). Create GPT. Compare estimate_params_from_config(d_model=128, num_layers=4, vocab_size=100) total vs model.count_parameters(non_embedding=False). They should match exactly (same formula).

Use `torch.no_grad()` context for all forward pass tests to save memory.
  </action>
  <verify>Run `cd /home/viggie/Projects/flops-fit && uv run pytest tests/test_model.py -v` -- all tests pass.</verify>
  <done>test_model.py has 15 tests covering GPTConfig, GPT forward pass, parameter counting, initialization, create_model_for_scaling, and estimate_params_from_config. All pass.</done>
</task>

</tasks>

<verification>
```bash
cd /home/viggie/Projects/flops-fit && uv run pytest tests/test_analyzer.py tests/test_model.py -v --tb=short
```
All analyzer and model tests pass.
</verification>

<success_criteria>
- test_analyzer.py has 12+ tests covering analyze(), predict(), bucket rounding, filtering
- test_model.py has 15 tests covering GPT forward/backward, param counting, initialization, scaling creation
- Characterization tests document known issues (bucket rounding mismatch, param estimation overshoot)
- All tests pass with `uv run pytest tests/ -v`
</success_criteria>

<output>
After completion, create `.planning/phases/01-existing-pipeline-baseline/01-02-SUMMARY.md`
</output>
