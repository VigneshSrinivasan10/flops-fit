---
phase: 08-vit-and-cifar-example
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/flops_fit/examples/vit.py
  - src/flops_fit/examples/cifar.py
  - src/flops_fit/examples/__init__.py
autonomous: true

must_haves:
  truths:
    - "VisionTransformer is importable from flops_fit.examples.vit with no import errors"
    - "VisionTransformer(embed_dim=64).num_params() returns a positive integer"
    - "Larger embed_dim produces larger num_params() (scaling works)"
    - "VisionTransformer forward pass accepts (B, 3, 32, 32) tensor and returns (B, 10) logits"
    - "CIFAR10Dataset is importable from flops_fit.examples.cifar"
    - "vit_loss_fn(logits, labels) returns a scalar (no tuple unpacking — differs from GPT)"
    - "VisionTransformer and CIFAR10Dataset are exported from flops_fit.examples.__init__"
  artifacts:
    - path: "src/flops_fit/examples/vit.py"
      provides: "VisionTransformer model class + vit_loss_fn"
      contains: "class VisionTransformer"
    - path: "src/flops_fit/examples/cifar.py"
      provides: "CIFAR10Dataset with lazy torchvision import"
      contains: "class CIFAR10Dataset"
    - path: "src/flops_fit/examples/__init__.py"
      provides: "Updated exports including VisionTransformer, vit_loss_fn, CIFAR10Dataset"
  key_links:
    - from: "src/flops_fit/examples/__init__.py"
      to: "src/flops_fit/examples/vit.py"
      via: "from flops_fit.examples.vit import VisionTransformer, vit_loss_fn"
      pattern: "from flops_fit.examples.vit import"
    - from: "src/flops_fit/examples/__init__.py"
      to: "src/flops_fit/examples/cifar.py"
      via: "from flops_fit.examples.cifar import CIFAR10Dataset"
      pattern: "from flops_fit.examples.cifar import"
---

<objective>
Create the ViT model class and CIFAR-10 dataset wrapper for Phase 8.

Purpose: Prove the library handles a second modality (images) with a different architecture (ViT vs GPT) and a simpler loss function (logits directly, not tuple).
Output: VisionTransformer (embed_dim as size param), CIFAR10Dataset (lazy torchvision load), vit_loss_fn, all exported from flops_fit.examples.
</objective>

<execution_context>
@/home/viggie/.claude/get-shit-done/workflows/execute-plan.md
@/home/viggie/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-gpt-and-tinystories-example/07-01-SUMMARY.md
@.planning/phases/08-vit-and-cifar-example/08-RESEARCH.md

# Existing files to follow as structural templates
@src/flops_fit/examples/__init__.py
@src/flops_fit/examples/gpt.py
@src/flops_fit/examples/tinystories.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create vit.py — VisionTransformer class and vit_loss_fn</name>
  <files>src/flops_fit/examples/vit.py</files>
  <action>
    Create src/flops_fit/examples/vit.py with:

    1. VisionTransformer(nn.Module):
       - Constructor args: embed_dim=256 (SIZE PARAM), image_size=32, patch_size=4, num_classes=10, num_layers=6, num_heads=8, mlp_dim=None (defaults to 4*embed_dim)
       - Add assertion: `assert image_size % patch_size == 0, f"patch_size {patch_size} must divide image_size {image_size}"`
       - Add assertion: `assert embed_dim % num_heads == 0, f"embed_dim {embed_dim} must be divisible by num_heads {num_heads}"`
       - self.num_patches = (image_size // patch_size) ** 2
       - self.patch_embed = nn.Linear(3 * patch_size * patch_size, embed_dim, bias=True)
       - self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
       - self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches + 1, embed_dim)) with nn.init.normal_(self.pos_embed, std=0.02)
       - self.encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=mlp_dim, batch_first=True, norm_first=True), num_layers=num_layers)
       - self.norm = nn.LayerNorm(embed_dim)
       - self.head = nn.Linear(embed_dim, num_classes)

    2. forward(self, x: torch.Tensor) -> torch.Tensor:
       - x shape: (B, 3, 32, 32) for CIFAR-10
       - Patch extraction using reshape+permute — exact pattern from RESEARCH.md:
         `patches = x.reshape(B, 3, image_size//patch_size, patch_size, image_size//patch_size, patch_size).permute(0, 2, 4, 1, 3, 5).reshape(B, self.num_patches, -1)`
       - x = self.patch_embed(patches)
       - cls = self.cls_token.expand(B, -1, -1); x = torch.cat([cls, x], dim=1)
       - x = x + self.pos_embed
       - x = self.encoder(x)
       - x = self.norm(x[:, 0])   # CLS token
       - return self.head(x)      # (B, num_classes) — logits DIRECTLY, not a tuple

    3. num_params(self) -> int:
       - return sum(p.numel() for p in self.parameters())

    4. vit_loss_fn(logits, labels) function (module level):
       - Docstring: "Image classification loss for ViT. Logits shape: (B, num_classes). Labels shape: (B,). Returns scalar cross-entropy. NOTE: unlike gpt_loss_fn, logits are direct (not a tuple)."
       - return F.cross_entropy(logits, labels)

    Key difference from GPT: forward() returns logits DIRECTLY (not a tuple). vit_loss_fn receives logits directly (not outputs tuple to unpack). This is a structural contrast with GPT that validates the library handles both patterns.

    Use: import torch; import torch.nn as nn; import torch.nn.functional as F
  </action>
  <verify>
    python -c "
    import torch
    from flops_fit.examples.vit import VisionTransformer, vit_loss_fn

    # Contract: num_params()
    small = VisionTransformer(embed_dim=64, num_layers=2, num_heads=8)
    large = VisionTransformer(embed_dim=128, num_layers=2, num_heads=8)
    assert small.num_params() > 0
    assert large.num_params() > small.num_params(), 'Larger embed_dim must produce more params'

    # Forward pass
    x = torch.randn(2, 3, 32, 32)
    logits = small(x)
    assert logits.shape == (2, 10), f'Expected (2, 10), got {logits.shape}'

    # Loss function (direct logits, not tuple)
    labels = torch.randint(0, 10, (2,))
    loss = vit_loss_fn(logits, labels)
    assert loss.ndim == 0, 'Loss must be scalar'
    assert loss.item() > 0
    print('vit.py OK')
    "
  </verify>
  <done>VisionTransformer(embed_dim=N) creates model; forward() returns (B,10) logits directly; num_params() returns int; vit_loss_fn(logits, labels) returns scalar. Assertion errors raised for incompatible patch_size or embed_dim/num_heads.</done>
</task>

<task type="auto">
  <name>Task 2: Create cifar.py and update examples/__init__.py exports</name>
  <files>src/flops_fit/examples/cifar.py, src/flops_fit/examples/__init__.py</files>
  <action>
    **Part A: src/flops_fit/examples/cifar.py**

    Create CIFAR10Dataset class following the same lazy-import pattern as tinystories.py:

    class CIFAR10Dataset(torch.utils.data.Dataset):
      - __init__(self, train=True, data_dir="./data"):
        - self.train = train
        - self.data_dir = data_dir
        - self._dataset = None   # lazy: not loaded until first access
      - _prepare_data(self):
        - If self._dataset is not None, return immediately
        - Lazy-import torchvision.datasets and torchvision.transforms here (NOT at module top)
        - Build transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2470, 0.2435, 0.2616))])
        - self._dataset = datasets.CIFAR10(root=self.data_dir, train=self.train, download=True, transform=transform)
      - __len__(self): _prepare_data(); return len(self._dataset)
      - __getitem__(self, idx): _prepare_data(); return self._dataset[idx]

    Key decisions:
    - Lazy-import torchvision inside _prepare_data() only (not at module top) — zero import-time overhead, matches tinystories.py pattern
    - download=True so users get data automatically on first access
    - Returns (image_tensor, label) pairs — image_tensor shape (3, 32, 32), label is int

    **Part B: src/flops_fit/examples/__init__.py**

    Update to add ViT exports alongside existing GPT exports:

    from flops_fit.examples.gpt import GPT, GPTConfig, create_model_for_scaling
    from flops_fit.examples.tinystories import TinyStoriesDataset
    from flops_fit.examples.vit import VisionTransformer, vit_loss_fn
    from flops_fit.examples.cifar import CIFAR10Dataset

    __all__ = [
        "GPT", "GPTConfig", "create_model_for_scaling",
        "TinyStoriesDataset",
        "VisionTransformer", "vit_loss_fn",
        "CIFAR10Dataset",
    ]
  </action>
  <verify>
    python -c "
    import torch
    # CIFAR10Dataset lazy-load check (no torchvision download at import)
    from flops_fit.examples.cifar import CIFAR10Dataset
    ds = CIFAR10Dataset(train=True)
    assert ds._dataset is None, 'Should not load at instantiation'
    print('CIFAR10Dataset lazy-load OK')

    # __init__.py exports check
    from flops_fit.examples import VisionTransformer, vit_loss_fn, CIFAR10Dataset
    print('examples/__init__.py exports OK')

    # Existing GPT exports still work
    from flops_fit.examples import GPT, GPTConfig, TinyStoriesDataset
    print('Backward-compat exports OK')
    "

    # All 188 existing tests still pass
    python -m pytest tests/ -x -q 2>&1 | tail -5
  </verify>
  <done>CIFAR10Dataset instantiates without network call (_dataset is None until __len__ or __getitem__). VisionTransformer, vit_loss_fn, CIFAR10Dataset importable from flops_fit.examples. All 188 existing tests still pass.</done>
</task>

</tasks>

<verification>
python -c "
from flops_fit.examples import VisionTransformer, vit_loss_fn, CIFAR10Dataset, GPT, GPTConfig, TinyStoriesDataset
import torch

# ViT contract
vit = VisionTransformer(embed_dim=64, num_layers=2, num_heads=8)
x = torch.randn(2, 3, 32, 32)
logits = vit(x)
assert logits.shape == (2, 10)
labels = torch.randint(0, 10, (2,))
loss = vit_loss_fn(logits, labels)
assert loss.ndim == 0

# CIFAR lazy
ds = CIFAR10Dataset()
assert ds._dataset is None

# GPT backward compat
gpt = GPT(GPTConfig(d_model=64, num_layers=2, num_heads=2, vocab_size=100, max_seq_len=32))
assert gpt.num_params() > 0
print('ALL OK')
"
python -m pytest tests/ -x -q 2>&1 | tail -3
</verification>

<success_criteria>
- VisionTransformer in vit.py: embed_dim size param, forward() returns (B,10) logits directly, num_params() method
- vit_loss_fn: takes (logits, labels) with no tuple unpacking — structurally different from gpt_loss_fn
- CIFAR10Dataset in cifar.py: lazy torchvision import, _dataset=None until first access
- flops_fit.examples.__init__ exports all 7 names (GPT, GPTConfig, create_model_for_scaling, TinyStoriesDataset, VisionTransformer, vit_loss_fn, CIFAR10Dataset)
- 188 existing tests still pass (no regressions)
</success_criteria>

<output>
After completion, create `.planning/phases/08-vit-and-cifar-example/08-01-SUMMARY.md`
</output>
