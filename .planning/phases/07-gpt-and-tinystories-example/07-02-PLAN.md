---
phase: 07-gpt-and-tinystories-example
plan: 02
type: execute
wave: 2
depends_on: ["07-01"]
files_modified:
  - src/flops_fit/api.py
  - src/flops_fit/examples/example_programmatic.py
  - src/flops_fit/examples/example_cli_wrapper.py
  - tests/test_examples.py
  - tests/test_api.py
autonomous: true

must_haves:
  truths:
    - "User can run `python -m flops_fit.examples.example_programmatic` and see scaling law output (mock mode)"
    - "User can run `python -m flops_fit.examples.example_cli_wrapper` and see a chinchilla table (mock mode)"
    - "Both example scripts work end-to-end with trainer mode=mock (no GPU, no network)"
    - "TinyStoriesDataset is mockable in tests (no HuggingFace download in CI)"
    - "find_optimal() accepts a `mode` parameter and passes it through to TrainingRunner"
    - "All 169 + new tests pass"
  artifacts:
    - path: "src/flops_fit/api.py"
      provides: "find_optimal() with mode parameter (default='local')"
      contains: "def find_optimal"
    - path: "src/flops_fit/examples/example_programmatic.py"
      provides: "Self-contained programmatic example of find_optimal() with GPT"
      contains: "flops_fit.find_optimal"
    - path: "src/flops_fit/examples/example_cli_wrapper.py"
      provides: "argparse-based CLI wrapper demonstrating library integration"
      contains: "argparse"
    - path: "tests/test_examples.py"
      provides: "Tests for examples package with mocked HuggingFace"
      contains: "mock"
  key_links:
    - from: "src/flops_fit/examples/example_programmatic.py"
      to: "flops_fit.find_optimal"
      via: "import flops_fit; flops_fit.find_optimal(..., mode='mock')"
      pattern: "find_optimal"
    - from: "src/flops_fit/examples/example_cli_wrapper.py"
      to: "flops_fit.find_optimal"
      via: "import flops_fit; flops_fit.find_optimal(..., mode='mock')"
      pattern: "find_optimal"
    - from: "src/flops_fit/api.py"
      to: "flops_fit.trainer.TrainingRunner"
      via: "TrainingRunner(mode=mode, ...)"
      pattern: "TrainingRunner\\(mode=mode"
    - from: "tests/test_examples.py"
      to: "flops_fit.examples.tinystories.TinyStoriesDataset"
      via: "unittest.mock.patch"
      pattern: "mock.*load_dataset|patch.*datasets"
---

<objective>
Add `mode` parameter to `find_optimal()` so example scripts can run in mock mode without GPU or network, then create two runnable example scripts and tests that verify everything works without real training infrastructure.

Purpose: These scripts are the primary user-facing deliverable of Phase 7. The `mode` parameter is required because `find_optimal()` currently hardcodes `TrainingRunner(mode="local")` — example scripts need `mode="mock"` to run quickly in demo/CI environments.

Output: Updated `api.py`, `example_programmatic.py`, `example_cli_wrapper.py`, and `tests/test_examples.py`.
</objective>

<execution_context>
@/home/viggie/.claude/get-shit-done/workflows/execute-plan.md
@/home/viggie/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@src/flops_fit/api.py
@src/flops_fit/examples/__init__.py
@.planning/phases/07-gpt-and-tinystories-example/07-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 0: Add `mode` parameter to find_optimal() in api.py</name>
  <files>
    src/flops_fit/api.py
    tests/test_api.py
  </files>
  <action>
    In `src/flops_fit/api.py`, add a `mode` parameter to `find_optimal()` and thread it through to `TrainingRunner`.

    Current signature (line 12):
    ```python
    def find_optimal(
        model_cls,
        model_size_param,
        model_kwargs=None,
        dataset=None,
        loss_fn=None,
        compute_budgets=None,
        train: bool = True,
        output_dir: str = "outputs",
        resume: bool = True,
        **kwargs,
    ):
    ```

    Updated signature — add `mode: str = "local"` before `output_dir`:
    ```python
    def find_optimal(
        model_cls,
        model_size_param,
        model_kwargs=None,
        dataset=None,
        loss_fn=None,
        compute_budgets=None,
        train: bool = True,
        mode: str = "local",
        output_dir: str = "outputs",
        resume: bool = True,
        **kwargs,
    ):
    ```

    Update the docstring `Args:` block to document the new parameter (add after `train:`):
    ```
        mode: Training runner mode. ``"local"`` runs real training (default,
            preserves existing behavior). ``"mock"`` uses a no-op runner that
            returns synthetic losses without GPU or data access — useful for
            testing and demo scripts.
    ```

    Update the `TrainingRunner` construction on line ~99 to pass `mode`:
    ```python
    # Before:
    runner = TrainingRunner(mode="local", output_dir=output_dir)
    # After:
    runner = TrainingRunner(mode=mode, output_dir=output_dir)
    ```

    No other changes to api.py. Default `mode="local"` preserves all existing behavior and tests.

    In `tests/test_api.py`, add a test that `mode` parameter is accepted and passed through.
    Read the existing test_api.py first, then append (do not overwrite) a new test class or test
    function confirming:
    - `find_optimal(..., mode="local", train=False)` returns SweepPlan (no regression)
    - `find_optimal()` signature accepts `mode` keyword without TypeError
    The simplest approach: add a test that calls `find_optimal(model_cls=..., model_size_param="d_model", train=False, compute_budgets=[1e12], mode="local")` and asserts it returns a SweepPlan. This is a non-training call so it exercises the parameter without needing TrainingRunner.
  </action>
  <verify>
    Run: `python -c "import inspect, flops_fit; sig = inspect.signature(flops_fit.find_optimal); assert 'mode' in sig.parameters, 'mode param missing'; print('mode param present:', sig.parameters['mode'].default)"` — should print `mode param present: local`.
    Run: `python -m pytest tests/test_api.py -x -q` — all existing + new tests pass.
  </verify>
  <done>
    `find_optimal()` accepts `mode` parameter with default `"local"`. `TrainingRunner` receives `mode=mode` (not hardcoded `"local"`). All existing api tests pass. New test for `mode` parameter passes.
  </done>
</task>

<task type="auto">
  <name>Task 1: Create programmatic and CLI wrapper example scripts</name>
  <files>
    src/flops_fit/examples/example_programmatic.py
    src/flops_fit/examples/example_cli_wrapper.py
  </files>
  <action>
    Create `src/flops_fit/examples/example_programmatic.py`:

    ```python
    #!/usr/bin/env python3
    """
    Example: Programmatic use of flops_fit.find_optimal() with GPT + TinyStories.

    This script demonstrates the full scaling law workflow:
    1. Define a model class using GPT from flops_fit.examples
    2. Load TinyStories dataset (or use mock mode without network)
    3. Call find_optimal() with compute budgets
    4. Display the Chinchilla scaling table

    Usage (mock mode, no GPU or network needed):
        python -m flops_fit.examples.example_programmatic

    Usage (real training, requires TinyStories download):
        python -m flops_fit.examples.example_programmatic --real

    The model factory wraps GPT so find_optimal() can vary d_model.
    The loss function reshapes GPT's (B, T, V) logits for cross_entropy.
    """

    import argparse
    import torch
    import torch.nn.functional as F

    import flops_fit
    from flops_fit.examples import GPT, GPTConfig, TinyStoriesDataset


    VOCAB_SIZE = 50257  # GPT-2 tokenizer vocab size


    def make_model_factory(num_layers: int = 4, num_heads: int = 4):
        """Return a factory function that creates GPT instances for a given d_model.

        find_optimal() calls model_cls(d_model=N) to create models at different
        sizes. This factory wraps GPTConfig instantiation so the caller only
        needs to vary d_model.

        Args:
            num_layers: Number of transformer layers (fixed across sweep).
            num_heads: Number of attention heads (fixed across sweep).

        Returns:
            Callable: (d_model: int) -> GPT instance with num_params() method.
        """
        def create_gpt(d_model: int) -> GPT:
            config = GPTConfig(
                d_model=d_model,
                num_layers=num_layers,
                num_heads=num_heads,
                vocab_size=VOCAB_SIZE,
                max_seq_len=256,
            )
            return GPT(config)
        return create_gpt


    def gpt_loss_fn(outputs, labels):
        """Language modeling loss for GPT outputs.

        GPT.forward() returns (logits, loss_or_None). This loss_fn is called
        by the trainer with (model_output, labels) where model_output is the
        raw return value of model.forward(input_ids).

        Args:
            outputs: Tuple of (logits, _) from GPT.forward(), logits shape (B, T, V).
            labels: Target token IDs, shape (B, T).

        Returns:
            Scalar cross-entropy loss.
        """
        logits, _ = outputs
        # Reshape for cross_entropy: (B*T, V) and (B*T,)
        return F.cross_entropy(
            logits.view(-1, VOCAB_SIZE),
            labels.view(-1),
        )


    def run(use_real_data: bool = False, output_dir: str = "outputs/gpt_tinystories"):
        """Run the scaling law experiment.

        Args:
            use_real_data: If True, downloads TinyStories from HuggingFace and
                           runs real local training (mode="local").
                           If False (default), uses trainer mock mode with
                           a minimal synthetic dataset (mode="mock").
            output_dir: Directory for results.json and plots.
        """
        print("=" * 60)
        print("flops_fit: GPT + TinyStories Scaling Law Example")
        print("=" * 60)

        # 1. Model factory — varies d_model, fixes other architecture params
        model_cls = make_model_factory(num_layers=4, num_heads=4)

        # 2. Dataset and training mode
        if use_real_data:
            print("Loading TinyStories dataset (requires internet)...")
            dataset = TinyStoriesDataset(split="train", seq_len=256)
            dataset.prepare_data()
            trainer_mode = "local"
        else:
            # Synthetic dataset for mock mode — avoids any network access
            print("Using synthetic dataset (mock mode, no download needed).")
            dataset = _make_synthetic_dataset(seq_len=256, size=128)
            trainer_mode = "mock"

        # 3. Small compute budgets for a quick demo (use 1e17+ for real experiments)
        compute_budgets = [1e12, 3e12, 1e13, 3e13, 1e14]

        print(f"\nRunning sweep over {len(compute_budgets)} compute budgets...")
        print(f"Trainer mode: {trainer_mode}")
        print(f"Output directory: {output_dir}\n")

        result = flops_fit.find_optimal(
            model_cls=model_cls,
            model_size_param="d_model",
            dataset=dataset,
            loss_fn=gpt_loss_fn,
            compute_budgets=compute_budgets,
            train=True,
            mode=trainer_mode,
            output_dir=output_dir,
        )

        # 4. Display results
        print("\n" + "=" * 60)
        print("SCALING LAW RESULTS (Chinchilla Table)")
        print("=" * 60)
        print(result.chinchilla_table())

        print("\nGenerating scaling law plots...")
        figs = result.plot(show=False)
        print(f"Saved {len(figs)} figure(s) to {output_dir}/plots/")

        return result


    def _make_synthetic_dataset(seq_len: int = 256, size: int = 128):
        """Create a tiny synthetic dataset for mock/CPU mode.

        Returns a torch Dataset yielding (input_ids, labels) pairs of random
        token IDs in [0, VOCAB_SIZE). Used when TinyStories download is not
        desired (testing, CI, quick demos).

        Args:
            seq_len: Sequence length for each example.
            size: Number of examples in the dataset.

        Returns:
            torch.utils.data.Dataset yielding (input_ids, labels) tensors.
        """
        from torch.utils.data import TensorDataset
        data = torch.randint(0, VOCAB_SIZE, (size, seq_len))
        return TensorDataset(data, data.clone())


    def main():
        parser = argparse.ArgumentParser(
            description="GPT + TinyStories scaling law example",
        )
        parser.add_argument(
            "--real",
            action="store_true",
            help="Use real TinyStories data + local training (requires internet). Default: synthetic mock data.",
        )
        parser.add_argument(
            "--output-dir",
            default="outputs/gpt_tinystories",
            help="Directory for results and plots.",
        )
        args = parser.parse_args()
        run(use_real_data=args.real, output_dir=args.output_dir)


    if __name__ == "__main__":
        main()
    ```

    ---

    Create `src/flops_fit/examples/example_cli_wrapper.py`:

    ```python
    #!/usr/bin/env python3
    """
    Example: CLI wrapper for flops_fit.find_optimal() using argparse.

    This demonstrates how to expose the library via command-line arguments.
    Users can adapt this pattern for their own model + dataset combinations.

    All library configuration is passed as arguments; no YAML files required.

    Usage:
        python -m flops_fit.examples.example_cli_wrapper --help
        python -m flops_fit.examples.example_cli_wrapper --budgets 1e12 3e12 1e13
        python -m flops_fit.examples.example_cli_wrapper --real --layers 6 --output-dir results/

    The key pattern:
        1. Parse args with argparse
        2. Build model_cls factory from arch params
        3. Build dataset from data params
        4. Call flops_fit.find_optimal() with compute_budgets from args
        5. Print results
    """

    import argparse
    import torch
    import torch.nn.functional as F

    import flops_fit
    from flops_fit.examples import GPT, GPTConfig, TinyStoriesDataset


    VOCAB_SIZE = 50257


    def make_model_factory(num_layers: int, num_heads: int):
        """Return a GPT factory callable for find_optimal().

        Args:
            num_layers: Fixed number of layers across the sweep.
            num_heads: Fixed number of attention heads.

        Returns:
            Callable: (d_model: int) -> GPT instance with num_params() method.
        """
        def create_gpt(d_model: int) -> GPT:
            config = GPTConfig(
                d_model=d_model,
                num_layers=num_layers,
                num_heads=num_heads,
                vocab_size=VOCAB_SIZE,
                max_seq_len=256,
            )
            return GPT(config)
        return create_gpt


    def gpt_loss_fn(outputs, labels):
        """Cross-entropy loss over GPT's (B, T, V) logits."""
        logits, _ = outputs
        return F.cross_entropy(logits.view(-1, VOCAB_SIZE), labels.view(-1))


    def build_parser() -> argparse.ArgumentParser:
        """Build the argument parser."""
        p = argparse.ArgumentParser(
            description="flops_fit: GPT + TinyStories scaling law CLI",
            formatter_class=argparse.ArgumentDefaultsHelpFormatter,
        )
        # Architecture
        p.add_argument("--layers", type=int, default=4, help="Number of transformer layers")
        p.add_argument("--heads", type=int, default=4, help="Number of attention heads")
        # Dataset
        p.add_argument("--real", action="store_true",
                       help="Use real TinyStories + local training (requires internet). Default: synthetic mock.")
        p.add_argument("--seq-len", type=int, default=256, help="Sequence length")
        p.add_argument("--cache-dir", default=".cache/datasets", help="HuggingFace cache directory")
        # Sweep
        p.add_argument(
            "--budgets", nargs="+", type=float,
            default=[1e12, 3e12, 1e13, 3e13, 1e14],
            help="Compute budgets in FLOPs",
        )
        # Output
        p.add_argument("--output-dir", default="outputs/gpt_tinystories", help="Results directory")
        return p


    def main():
        args = build_parser().parse_args()

        print("=" * 60)
        print("flops_fit CLI: GPT + TinyStories")
        print("=" * 60)
        print(f"Architecture: {args.layers} layers, {args.heads} heads")
        print(f"Budgets: {args.budgets}")
        print(f"Dataset: {'TinyStories (real)' if args.real else 'Synthetic (mock)'}")
        print()

        # Model factory
        model_cls = make_model_factory(num_layers=args.layers, num_heads=args.heads)

        # Dataset and training mode
        if args.real:
            print("Loading TinyStories dataset...")
            dataset = TinyStoriesDataset(
                split="train",
                seq_len=args.seq_len,
                cache_dir=args.cache_dir,
            )
            dataset.prepare_data()
            trainer_mode = "local"
        else:
            from torch.utils.data import TensorDataset
            print("Using synthetic dataset (no download).")
            data = torch.randint(0, VOCAB_SIZE, (128, args.seq_len))
            dataset = TensorDataset(data, data.clone())
            trainer_mode = "mock"

        # Run find_optimal — pass mode so TrainingRunner uses the right backend
        result = flops_fit.find_optimal(
            model_cls=model_cls,
            model_size_param="d_model",
            dataset=dataset,
            loss_fn=gpt_loss_fn,
            compute_budgets=args.budgets,
            train=True,
            mode=trainer_mode,
            output_dir=args.output_dir,
        )

        print(result.chinchilla_table())


    if __name__ == "__main__":
        main()
    ```

    Both scripts pass `mode=trainer_mode` to `find_optimal()`. Default (no `--real` flag) uses
    `mode="mock"` so demos and CI runs complete quickly without GPU or data download.
    Both scripts handle the GPT loss correctly: `logits.view(-1, VOCAB_SIZE)` and `labels.view(-1)`.
  </action>
  <verify>
    Run: `python -m flops_fit.examples.example_programmatic --help` — prints help without error.
    Run: `python -m flops_fit.examples.example_cli_wrapper --help` — prints help without error.
    (Full end-to-end tested via test_examples.py in Task 2.)
  </verify>
  <done>
    Both scripts import without error. `--help` works for both. Both pass `mode="mock"` by default. Loss function shapes are correct (view(-1, VOCAB_SIZE)).
  </done>
</task>

<task type="auto">
  <name>Task 2: Create tests/test_examples.py with mocked HuggingFace</name>
  <files>tests/test_examples.py</files>
  <action>
    Create `tests/test_examples.py` with tests covering:
    - TinyStoriesDataset instantiates without network (no HuggingFace import at class level)
    - TinyStoriesDataset with mocked HuggingFace returns correct tensor shapes
    - GPT has num_params() method returning positive int (contract method checked by model_factory)
    - Programmatic example runs end-to-end with synthetic dataset (no mocking needed — uses synthetic _make_synthetic_dataset path)
    - CLI wrapper main() function parses args and calls find_optimal (smoke test, mocked)

    ```python
    """Tests for flops_fit.examples package.

    TinyStories tests mock HuggingFace to avoid network access in CI.
    Example script tests use synthetic datasets (no mocking needed for data).
    """

    import pytest
    import torch
    from unittest.mock import patch, MagicMock


    # ---------------------------------------------------------------------------
    # GPT contract
    # ---------------------------------------------------------------------------

    class TestGPTContract:
        """GPT in examples satisfies the flops_fit model contract."""

        def test_num_params_method_exists(self):
            """GPT.num_params() exists and returns a positive integer.

            num_params() is the primary contract method checked by
            model_factory.validate_model_contract(). NOT n().
            """
            from flops_fit.examples import GPT, GPTConfig
            config = GPTConfig(d_model=64, num_layers=2, num_heads=2, vocab_size=100, max_seq_len=32)
            model = GPT(config)
            result = model.num_params()
            assert isinstance(result, int)
            assert result > 0

        def test_num_params_increases_with_d_model(self):
            """Larger d_model produces larger num_params()."""
            from flops_fit.examples import GPT, GPTConfig
            small = GPT(GPTConfig(d_model=64, num_layers=2, num_heads=2, vocab_size=100, max_seq_len=32))
            large = GPT(GPTConfig(d_model=128, num_layers=2, num_heads=2, vocab_size=100, max_seq_len=32))
            assert large.num_params() > small.num_params()

        def test_backward_compat_import_from_model_py(self):
            """GPT can still be imported from flops_fit.model (backward compat)."""
            from flops_fit.model import GPT, GPTConfig
            config = GPTConfig(d_model=64, num_layers=2, num_heads=2, vocab_size=100, max_seq_len=32)
            model = GPT(config)
            assert model.num_params() > 0


    # ---------------------------------------------------------------------------
    # TinyStoriesDataset: lazy loading
    # ---------------------------------------------------------------------------

    class TestTinyStoriesDatasetLazyLoad:
        """TinyStoriesDataset does not trigger network access at import or instantiation."""

        def test_import_is_instant(self):
            """TinyStoriesDataset imports without HuggingFace side effects."""
            # If this import triggered a download, CI would hang.
            from flops_fit.examples.tinystories import TinyStoriesDataset
            assert TinyStoriesDataset is not None

        def test_instantiation_does_not_call_load_dataset(self):
            """TinyStoriesDataset() does not call load_dataset at __init__."""
            from flops_fit.examples.tinystories import TinyStoriesDataset
            with patch("flops_fit.examples.tinystories.TinyStoriesDataset.prepare_data") as mock_prepare:
                ds = TinyStoriesDataset(split="train", seq_len=128)
                mock_prepare.assert_not_called()

        def test_attributes_stored(self):
            """Constructor stores all attributes for later use."""
            from flops_fit.examples.tinystories import TinyStoriesDataset
            ds = TinyStoriesDataset(split="validation", seq_len=64, cache_dir="/tmp/cache", tokenizer_name="gpt2")
            assert ds.split == "validation"
            assert ds.seq_len == 64
            assert ds.cache_dir == "/tmp/cache"
            assert ds.tokenizer_name == "gpt2"
            assert ds._dataset is None
            assert ds._tokenizer is None


    # ---------------------------------------------------------------------------
    # TinyStoriesDataset: mocked HuggingFace
    # ---------------------------------------------------------------------------

    def _make_mock_hf_dataset(stories, seq_len):
        """Build a minimal mock HuggingFace dataset-like object."""
        records = [{"story": s} for s in stories]
        mock_ds = MagicMock()
        mock_ds.__len__ = MagicMock(return_value=len(records))
        mock_ds.__getitem__ = MagicMock(side_effect=lambda i: records[i])
        return mock_ds


    def _make_mock_tokenizer(seq_len):
        """Build a mock HuggingFace tokenizer."""
        mock_tok = MagicMock()
        mock_tok.pad_token = mock_tok.eos_token  # already set
        # When called as tokenizer(text, ...) returns encoded dict
        def tokenize(text, max_length=seq_len, truncation=True, padding="max_length", return_tensors="pt"):
            ids = torch.zeros(1, max_length, dtype=torch.long)
            return {"input_ids": ids}
        mock_tok.side_effect = tokenize
        return mock_tok


    class TestTinyStoriesDatasetWithMock:
        """TinyStoriesDataset behavior with mocked HuggingFace."""

        @pytest.fixture
        def ds_with_mock(self):
            """Return a TinyStoriesDataset with HuggingFace mocked."""
            from flops_fit.examples.tinystories import TinyStoriesDataset

            stories = ["Once upon a time.", "The end.", "A story."]
            seq_len = 32

            mock_hf_ds = _make_mock_hf_dataset(stories, seq_len)
            mock_tok = _make_mock_tokenizer(seq_len)

            ds = TinyStoriesDataset(split="train", seq_len=seq_len)

            # Directly inject mocked objects (bypasses HuggingFace)
            ds._dataset = mock_hf_ds
            ds._tokenizer = mock_tok

            return ds, stories, seq_len

        def test_len_returns_dataset_length(self, ds_with_mock):
            ds, stories, seq_len = ds_with_mock
            assert len(ds) == len(stories)

        def test_getitem_returns_tensor_pair(self, ds_with_mock):
            ds, stories, seq_len = ds_with_mock
            input_ids, labels = ds[0]
            assert isinstance(input_ids, torch.Tensor)
            assert isinstance(labels, torch.Tensor)
            assert input_ids.shape == (seq_len,)
            assert labels.shape == (seq_len,)

        def test_labels_equal_input_ids(self, ds_with_mock):
            ds, stories, seq_len = ds_with_mock
            input_ids, labels = ds[0]
            assert torch.equal(input_ids, labels)

        def test_dtype_is_long(self, ds_with_mock):
            ds, stories, seq_len = ds_with_mock
            input_ids, labels = ds[0]
            assert input_ids.dtype == torch.long
            assert labels.dtype == torch.long


    # ---------------------------------------------------------------------------
    # Example scripts: smoke tests (no HuggingFace, no GPU)
    # ---------------------------------------------------------------------------

    class TestProgrammaticExample:
        """Smoke tests for the programmatic example script."""

        def test_make_model_factory_creates_gpt_with_num_params(self):
            """make_model_factory returns a callable that produces GPT with num_params()."""
            from flops_fit.examples.example_programmatic import make_model_factory
            factory = make_model_factory(num_layers=2, num_heads=2)
            model = factory(d_model=64)
            assert hasattr(model, "num_params")
            assert model.num_params() > 0

        def test_make_synthetic_dataset_shape(self):
            """_make_synthetic_dataset returns Dataset with correct shape."""
            from flops_fit.examples.example_programmatic import _make_synthetic_dataset
            ds = _make_synthetic_dataset(seq_len=16, size=8)
            assert len(ds) == 8
            ids, labels = ds[0]
            assert ids.shape == (16,)
            assert labels.shape == (16,)

        def test_gpt_loss_fn_shape(self):
            """gpt_loss_fn returns scalar given correct shapes."""
            from flops_fit.examples.example_programmatic import gpt_loss_fn, VOCAB_SIZE
            B, T = 2, 8
            logits = torch.randn(B, T, VOCAB_SIZE)
            labels = torch.randint(0, VOCAB_SIZE, (B, T))
            outputs = (logits, None)
            loss = gpt_loss_fn(outputs, labels)
            assert loss.shape == ()  # scalar
            assert loss.item() > 0


    class TestCLIWrapperExample:
        """Smoke tests for the CLI wrapper example script."""

        def test_build_parser(self):
            """build_parser() creates a valid ArgumentParser with expected args."""
            from flops_fit.examples.example_cli_wrapper import build_parser
            parser = build_parser()
            # Parse empty args — should use defaults
            args = parser.parse_args([])
            assert args.layers == 4
            assert args.heads == 4
            assert not args.real
            assert args.seq_len == 256

        def test_make_model_factory(self):
            """make_model_factory in CLI wrapper creates GPT with num_params()."""
            from flops_fit.examples.example_cli_wrapper import make_model_factory
            factory = make_model_factory(num_layers=2, num_heads=2)
            model = factory(d_model=64)
            assert model.num_params() > 0

        def test_gpt_loss_fn_cli(self):
            """gpt_loss_fn in CLI wrapper returns scalar."""
            from flops_fit.examples.example_cli_wrapper import gpt_loss_fn, VOCAB_SIZE
            B, T = 2, 8
            logits = torch.randn(B, T, VOCAB_SIZE)
            labels = torch.randint(0, VOCAB_SIZE, (B, T))
            loss = gpt_loss_fn((logits, None), labels)
            assert loss.ndim == 0  # scalar
    ```

    Run the full test suite after writing to verify no regressions: `python -m pytest tests/ -x -q`.
  </action>
  <verify>
    Run: `python -m pytest tests/test_examples.py -v`
    All test classes should pass:
    - TestGPTContract (3 tests — uses num_params() not n())
    - TestTinyStoriesDatasetLazyLoad (3 tests)
    - TestTinyStoriesDatasetWithMock (4 tests)
    - TestProgrammaticExample (3 tests — uses num_params() not n())
    - TestCLIWrapperExample (3 tests — uses num_params() not n())

    Run: `python -m pytest tests/ -x -q` — all 169 + new tests pass.
  </verify>
  <done>
    All tests in test_examples.py pass. Full suite (169 + new) passes. No HuggingFace network calls in CI. GPT contract (num_params() method) verified via tests.
  </done>
</task>

</tasks>

<verification>
1. `python -m flops_fit.examples.example_programmatic --help` exits 0.
2. `python -m flops_fit.examples.example_cli_wrapper --help` exits 0.
3. `python -m pytest tests/test_examples.py -v` — all tests pass without network access.
4. `python -m pytest tests/ -q` — full suite passes (169 + new tests).
5. `python -c "import inspect, flops_fit; assert 'mode' in inspect.signature(flops_fit.find_optimal).parameters"` exits 0.
</verification>

<success_criteria>
1. Both example scripts are runnable (`--help` works, no import errors).
2. Both example scripts default to `mode="mock"` (no `--real` flag) so demos run without GPU or network.
3. `find_optimal()` in api.py accepts `mode` parameter (default `"local"`) and passes it to `TrainingRunner`.
4. `test_examples.py` covers GPT contract (`num_params()`), TinyStories lazy-loading, mocked dataset, and example script smoke tests.
5. No HuggingFace downloads triggered during `pytest tests/test_examples.py`.
6. Full test suite passes without regressions.
7. Phase 7 success criteria satisfied:
   - EX-01: `example_programmatic.py` shows find_optimal() with GPT + TinyStories.
   - EX-03: `example_cli_wrapper.py` shows CLI wrapper pattern.
   - GPT importable from `flops_fit.examples.gpt`.
   - Running the example end-to-end produces scaling law predictions (in mock or CPU mode).
</success_criteria>

<output>
After completion, create `.planning/phases/07-gpt-and-tinystories-example/07-02-SUMMARY.md`
</output>
