---
phase: 07-gpt-and-tinystories-example
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/flops_fit/examples/__init__.py
  - src/flops_fit/examples/gpt.py
  - src/flops_fit/examples/tinystories.py
  - src/flops_fit/model.py
  - src/flops_fit/__init__.py
autonomous: true

must_haves:
  truths:
    - "User can run `from flops_fit.examples import GPT, GPTConfig, TinyStoriesDataset` without error"
    - "GPT instance satisfies library contract: `gpt.n()` returns a positive integer"
    - "TinyStoriesDataset can be instantiated without downloading (lazy-load pattern)"
    - "TinyStoriesDataset.__len__ and __getitem__ work when dataset is loaded"
    - "Existing `from flops_fit.model import GPT, GPTConfig` still works (backward compat re-export)"
    - "Existing 169 tests still pass after refactor"
  artifacts:
    - path: "src/flops_fit/examples/__init__.py"
      provides: "Package exports: GPT, GPTConfig, TinyStoriesDataset"
      exports: ["GPT", "GPTConfig", "TinyStoriesDataset"]
    - path: "src/flops_fit/examples/gpt.py"
      provides: "GPT model with n() contract method"
      contains: "def n(self)"
    - path: "src/flops_fit/examples/tinystories.py"
      provides: "Lazy-loading TinyStories PyTorch Dataset"
      contains: "class TinyStoriesDataset"
  key_links:
    - from: "src/flops_fit/model.py"
      to: "src/flops_fit/examples/gpt.py"
      via: "re-export"
      pattern: "from flops_fit.examples.gpt import"
    - from: "src/flops_fit/__init__.py"
      to: "src/flops_fit/examples/__init__.py"
      via: "import"
      pattern: "from flops_fit.examples import"
---

<objective>
Create the `flops_fit.examples` package by moving GPT to examples, adding the `n()` contract method, creating the TinyStories dataset wrapper, and maintaining backward compatibility.

Purpose: This is the structural foundation that enables Phase 7 examples. The examples package makes GPT importable as a reference implementation and provides TinyStories as a ready-made dataset for the example scripts.

Output: `src/flops_fit/examples/` package with GPT (+ n() method), TinyStoriesDataset wrapper, and backward-compat re-exports in model.py.
</objective>

<execution_context>
@/home/viggie/.claude/get-shit-done/workflows/execute-plan.md
@/home/viggie/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@src/flops_fit/model.py
@src/flops_fit/__init__.py
@src/flops_fit/model_factory.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create examples package with GPT (moved + n() added)</name>
  <files>
    src/flops_fit/examples/__init__.py
    src/flops_fit/examples/gpt.py
    src/flops_fit/model.py
    src/flops_fit/__init__.py
  </files>
  <action>
    1. Create `src/flops_fit/examples/` directory.

    2. Create `src/flops_fit/examples/gpt.py` by copying the ENTIRE contents of `src/flops_fit/model.py` into it. Do NOT delete anything from the copy — preserve all classes (GPTConfig, RMSNorm, RotaryEmbedding, CausalSelfAttention, FeedForward, TransformerBlock, GPT) and all module-level functions (rotate_half, apply_rotary_pos_emb, estimate_model_flops, estimate_params_from_config, create_model_for_scaling).

    3. In `src/flops_fit/examples/gpt.py`, add the `n()` method to the `GPT` class (library contract). Place it immediately after `count_parameters()` (line ~387 in original):
       ```python
       def n(self) -> int:
           """Return total number of parameters (flops_fit contract)."""
           return sum(p.numel() for p in self.parameters())
       ```

    4. Update `src/flops_fit/model.py` to re-export from examples for backward compatibility. Replace the entire content of `model.py` with a thin re-export module:
       ```python
       """flops-fit Model Implementation - backward compatibility re-export.

       The canonical GPT implementation lives in flops_fit.examples.gpt.
       This module re-exports all public symbols for backward compatibility.
       """
       from flops_fit.examples.gpt import (  # noqa: F401
           GPTConfig,
           RMSNorm,
           RotaryEmbedding,
           rotate_half,
           apply_rotary_pos_emb,
           CausalSelfAttention,
           FeedForward,
           TransformerBlock,
           GPT,
           estimate_model_flops,
           estimate_params_from_config,
           create_model_for_scaling,
       )
       ```

    5. Create `src/flops_fit/examples/__init__.py`:
       ```python
       """flops_fit.examples: Reference implementations for library users.

       Provides GPT + TinyStories as ready-made model + dataset for
       demonstrating flops_fit.find_optimal() usage.
       """
       from flops_fit.examples.gpt import GPT, GPTConfig, create_model_for_scaling
       from flops_fit.examples.tinystories import TinyStoriesDataset

       __all__ = [
           "GPT",
           "GPTConfig",
           "create_model_for_scaling",
           "TinyStoriesDataset",
       ]
       ```

    6. Update `src/flops_fit/__init__.py` to also import from examples (add after existing model imports):
       ```python
       from flops_fit.examples import GPT, GPTConfig, TinyStoriesDataset
       ```
       Add `"TinyStoriesDataset"` to `__all__`. Keep existing `GPT, GPTConfig` entries since they now resolve through examples via model.py re-export — but update the import line to use `from flops_fit.model import ...` (which re-exports from examples) to avoid any circular import risk.

    NOTE: Circular import risk — `examples/gpt.py` must NOT import from `flops_fit` core modules (api.py, trainer.py, etc.). It only uses `torch`, `torch.nn`, `math`, `collections`, `dataclasses`, `typing`. Verify no such imports exist in the copied code.
  </action>
  <verify>
    Run: `python -c "from flops_fit.model import GPT, GPTConfig; m = GPT(GPTConfig(d_model=64, num_layers=2, num_heads=2)); print(m.n())"` — should print a positive integer.
    Run: `python -c "from flops_fit.examples import GPT, GPTConfig, TinyStoriesDataset; print('OK')"` — should print "OK".
    Run: `python -m pytest tests/test_model.py -x -q` — all model tests should pass.
  </verify>
  <done>
    `from flops_fit.model import GPT` and `from flops_fit.examples import GPT` both work. GPT instance has `n()` returning positive int. Existing model tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create TinyStoriesDataset in examples/tinystories.py</name>
  <files>src/flops_fit/examples/tinystories.py</files>
  <action>
    Create `src/flops_fit/examples/tinystories.py` with a lazy-loading PyTorch Dataset wrapper for TinyStories. The HuggingFace dataset must NOT be downloaded at import time — download happens only when `.prepare_data()` is called explicitly or when `__getitem__` is called for the first time.

    ```python
    """TinyStories dataset wrapper for flops_fit examples.

    Provides a PyTorch Dataset wrapper around the HuggingFace TinyStories dataset
    (roneneldan/TinyStories) for use with flops_fit.find_optimal().

    Usage:
        dataset = TinyStoriesDataset(split="train", seq_len=256)
        dataset.prepare_data()  # Downloads and caches HuggingFace dataset
        input_ids, labels = dataset[0]  # Returns (seq_len,) tensors

    Note: Requires internet access on first run to download from HuggingFace.
          Data is cached to cache_dir (default: .cache/datasets) for subsequent runs.
    """

    from typing import Optional, Tuple
    import torch
    from torch.utils.data import Dataset


    class TinyStoriesDataset(Dataset):
        """PyTorch Dataset wrapper for HuggingFace roneneldan/TinyStories.

        Lazy-loads dataset from HuggingFace on first access. Caches to disk
        to avoid repeated downloads.

        Args:
            split: Dataset split — "train" or "validation".
            seq_len: Sequence length for language modeling. Sequences are
                truncated to seq_len tokens. Shorter sequences are left-padded
                with the EOS token ID.
            cache_dir: HuggingFace cache directory for downloaded datasets
                and tokenizers. Defaults to ".cache/datasets".
            tokenizer_name: HuggingFace tokenizer name. Defaults to "gpt2"
                (vocab_size=50257, matches GPT model default).
        """

        def __init__(
            self,
            split: str = "train",
            seq_len: int = 256,
            cache_dir: str = ".cache/datasets",
            tokenizer_name: str = "gpt2",
        ):
            self.split = split
            self.seq_len = seq_len
            self.cache_dir = cache_dir
            self.tokenizer_name = tokenizer_name
            # Lazy: do NOT load dataset or tokenizer here
            self._dataset = None
            self._tokenizer = None

        def prepare_data(self) -> None:
            """Download and cache the dataset and tokenizer.

            Call this explicitly before training to ensure data is available.
            If not called, __getitem__ calls it on first access.
            Prints progress so users know it is not hung.
            """
            if self._dataset is not None:
                return
            # Import lazily to avoid import-time side effects
            from datasets import load_dataset
            from transformers import AutoTokenizer

            print(f"Loading tokenizer '{self.tokenizer_name}'...")
            self._tokenizer = AutoTokenizer.from_pretrained(
                self.tokenizer_name,
                cache_dir=self.cache_dir,
            )
            if self._tokenizer.pad_token is None:
                self._tokenizer.pad_token = self._tokenizer.eos_token

            print(f"Loading TinyStories ({self.split} split) from HuggingFace...")
            self._dataset = load_dataset(
                "roneneldan/TinyStories",
                split=self.split,
                cache_dir=self.cache_dir,
                trust_remote_code=False,
            )
            print(f"TinyStories loaded: {len(self._dataset)} examples.")

        def __len__(self) -> int:
            """Return number of examples. Loads dataset if not yet loaded."""
            if self._dataset is None:
                self.prepare_data()
            return len(self._dataset)

        def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
            """Return (input_ids, labels) tensors for language modeling.

            Both tensors have shape (seq_len,). Labels equal input_ids (the
            trainer is responsible for next-token shifting if needed).

            Args:
                idx: Example index.

            Returns:
                Tuple of (input_ids, labels), both shape (seq_len,), dtype torch.long.
            """
            if self._dataset is None:
                self.prepare_data()

            story = self._dataset[idx]["story"]

            encoded = self._tokenizer(
                story,
                max_length=self.seq_len,
                truncation=True,
                padding="max_length",
                return_tensors="pt",
            )

            input_ids = encoded["input_ids"].squeeze(0)  # (seq_len,)
            labels = input_ids.clone()

            return input_ids, labels
    ```

    No other imports at module level besides typing and standard torch. `datasets` and `transformers` are only imported inside `prepare_data()`.
  </action>
  <verify>
    Run: `python -c "from flops_fit.examples.tinystories import TinyStoriesDataset; ds = TinyStoriesDataset(); print('instantiated OK — no download triggered')"` — must complete instantly (no network call).
    Run: `python -m pytest tests/test_examples.py -x -q -k tinystories` (written in Plan 02) — mock-based tests pass.
  </verify>
  <done>
    `TinyStoriesDataset()` instantiates without network access. `prepare_data()` is the only entry point that triggers download. Class satisfies `torch.utils.data.Dataset` interface (`__len__`, `__getitem__`).
  </done>
</task>

</tasks>

<verification>
Run full test suite: `python -m pytest tests/ -x -q`
Expected: All 169 existing tests pass (no regressions from model.py re-export change).
Run import smoke test: `python -c "from flops_fit.examples import GPT, GPTConfig, TinyStoriesDataset; from flops_fit.model import GPT, GPTConfig; import flops_fit; print(flops_fit.__version__)"`
</verification>

<success_criteria>
1. `from flops_fit.examples import GPT, GPTConfig, TinyStoriesDataset` succeeds.
2. `from flops_fit.model import GPT, GPTConfig` succeeds (backward compat re-export).
3. `GPT(GPTConfig(d_model=64, num_layers=2, num_heads=2)).n()` returns a positive integer.
4. `TinyStoriesDataset()` instantiates instantly without network calls.
5. All 169 pre-existing tests pass.
</success_criteria>

<output>
After completion, create `.planning/phases/07-gpt-and-tinystories-example/07-01-SUMMARY.md`
</output>
