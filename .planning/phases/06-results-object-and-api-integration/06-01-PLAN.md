---
phase: 06-results-object-and-api-integration
plan: 01
type: tdd
wave: 1
depends_on: []
files_modified:
  - tests/test_result.py
  - src/flops_fit/result.py
autonomous: true

must_haves:
  truths:
    - "Result.chinchilla_table() returns a markdown table string"
    - "Result.predict(compute_budget) returns a dict with optimal_params, optimal_tokens, expected_loss, tokens_per_param"
    - "Result.plot() returns a list of matplotlib Figure objects"
    - "Result is importable from flops_fit.result"
  artifacts:
    - path: "src/flops_fit/result.py"
      provides: "Result dataclass with chinchilla_table, predict, plot methods"
      exports: ["Result"]
    - path: "tests/test_result.py"
      provides: "TDD tests for Result methods"
      contains: "class TestResult"
  key_links:
    - from: "src/flops_fit/result.py"
      to: "src/flops_fit/analyzer.py"
      via: "Result.analysis field (ScalingAnalysis instance)"
      pattern: "self\\.analysis\\.chinchilla_table|self\\.analysis\\.predict_optimal_size"
    - from: "src/flops_fit/result.py"
      to: "src/flops_fit/visualizer.py"
      via: "Result.visualizer field (ScalingVisualizer instance)"
      pattern: "self\\.visualizer\\.plot_all"
---

<objective>
TDD: Create Result dataclass that wraps ScalingAnalysis + ScalingVisualizer and exposes three user-facing methods.

Purpose: Result is the user-visible object returned from find_optimal(). All method logic delegates to Phase 5 components; Result is a facade, not a reimplementation.
Output: tests/test_result.py (failing then passing), src/flops_fit/result.py
</objective>

<execution_context>
@/home/viggie/.claude/get-shit-done/workflows/execute-plan.md
@/home/viggie/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@src/flops_fit/analyzer.py
@src/flops_fit/visualizer.py
</context>

<tasks>

<task type="tdd">
  <name>Task 1 (RED): Write failing tests for Result dataclass</name>
  <files>tests/test_result.py</files>
  <action>
Create tests/test_result.py with:

1. Import guard at top:
```python
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt
import pytest
import numpy as np
from flops_fit.result import Result
from flops_fit.analyzer import ScalingAnalysis, PowerLawFit
from flops_fit.visualizer import ScalingVisualizer
```

2. Fixture `scaling_analysis` — create a minimal but realistic ScalingAnalysis using PowerLawFit instances:
```python
@pytest.fixture
def scaling_analysis():
    n_fit = PowerLawFit(name="N_opt", coefficient_k=1e-3, exponent_a=0.5, r_squared=0.99, l_inf=0.0)
    d_fit = PowerLawFit(name="D_opt", coefficient_k=2e-3, exponent_a=0.5, r_squared=0.99, l_inf=0.0)
    l_fit = PowerLawFit(name="L_opt", coefficient_k=5.0, exponent_a=-0.1, r_squared=0.95, l_inf=1.5)
    return ScalingAnalysis(
        n_opt_fit=n_fit,
        d_opt_fit=d_fit,
        l_opt_fit=l_fit,
        optimal_points=[],
        optimal_ratio=20.0,
    )
```

3. Fixture `scaling_visualizer` — use tmp_path to create a ScalingVisualizer pointing at a fake results.json and analysis JSON (create minimal valid JSON files so load_data() won't fail):
```python
@pytest.fixture
def scaling_visualizer(tmp_path):
    import json
    results = [
        {"status": "completed", "compute_budget": 1e18, "model_size": 1000,
         "num_tokens": 20000, "final_loss": 2.5, "experiment_id": "e1",
         "actual_flops": 1e18, "wall_time_seconds": 1.0}
    ]
    (tmp_path / "results.json").write_text(json.dumps(results))
    analysis = {
        "n_opt_fit": {"coefficient_k": 1e-3, "exponent_a": 0.5},
        "d_opt_fit": {"coefficient_k": 2e-3, "exponent_a": 0.5},
        "l_opt_fit": {"coefficient_k": 5.0, "exponent_a": -0.1},
    }
    (tmp_path / "scaling_laws.json").write_text(json.dumps(analysis))
    return ScalingVisualizer(
        results_path=tmp_path / "results.json",
        analysis_path=tmp_path / "scaling_laws.json",
        output_dir=tmp_path / "plots",
    )
```

4. Fixture `result` combining both:
```python
@pytest.fixture
def result(scaling_analysis, scaling_visualizer, tmp_path):
    return Result(
        analysis=scaling_analysis,
        visualizer=scaling_visualizer,
        output_dir=str(tmp_path),
        compute_budgets=[1e18, 1e19],
    )
```

5. Autouse fixture to close matplotlib figures:
```python
@pytest.fixture(autouse=True)
def cleanup_matplotlib():
    yield
    plt.close("all")
    plt.rcdefaults()
```

6. class TestResult with these tests:
- test_result_is_importable: `from flops_fit.result import Result; assert Result`
- test_chinchilla_table_returns_string: `table = result.chinchilla_table(); assert isinstance(table, str); assert "Compute Budget" in table; assert "|" in table`
- test_chinchilla_table_with_custom_budgets: `table = result.chinchilla_table([1e18, 1e20]); assert table.count("|---") == 1`  (one separator row = 2 data rows)
- test_predict_returns_dict: `pred = result.predict(1e18); assert isinstance(pred, dict); assert set(pred.keys()) >= {"optimal_params", "optimal_tokens", "expected_loss", "tokens_per_param"}`
- test_predict_returns_numeric_values: `pred = result.predict(1e18); assert pred["optimal_params"] > 0; assert pred["optimal_tokens"] > 0; assert pred["expected_loss"] > 0`
- test_plot_returns_figures: `figs = result.plot(); assert isinstance(figs, list); assert len(figs) > 0; import matplotlib.pyplot as plt; assert all(isinstance(f, plt.Figure) for f in figs)`
- test_plot_saves_to_output_dir: `from pathlib import Path; result.plot(); plots_dir = Path(result.output_dir) / "plots"; assert any(plots_dir.glob("*.png"))`
- test_result_stores_compute_budgets: `assert result.compute_budgets == [1e18, 1e19]`
- test_result_stores_output_dir: `assert result.output_dir is not None`

Run: `uv run pytest tests/test_result.py -v` — all tests MUST fail with ImportError or AttributeError (result.py does not exist yet). Commit: `test(06-01): add failing tests for Result dataclass`
  </action>
  <verify>uv run pytest tests/test_result.py -v 2>&1 | grep -E "FAILED|ERROR|ImportError" | head -20</verify>
  <done>All tests in test_result.py fail with ImportError (result.py doesn't exist). RED phase confirmed.</done>
</task>

<task type="tdd">
  <name>Task 2 (GREEN): Implement Result dataclass</name>
  <files>src/flops_fit/result.py</files>
  <action>
Create src/flops_fit/result.py:

```python
"""Result object returned from find_optimal().

Wraps ScalingAnalysis and ScalingVisualizer to expose a cohesive
user-facing API after running the full scaling law pipeline.
"""

from __future__ import annotations

from dataclasses import dataclass, field
from pathlib import Path
from typing import TYPE_CHECKING

import matplotlib.pyplot as plt

if TYPE_CHECKING:
    from flops_fit.analyzer import ScalingAnalysis
    from flops_fit.visualizer import ScalingVisualizer


@dataclass
class Result:
    """
    Results from find_optimal() pipeline.

    Aggregates power law analysis and visualization capability.
    All heavy lifting is delegated to ScalingAnalysis (fitting)
    and ScalingVisualizer (plotting) from Phase 5.

    Attributes:
        analysis: Fitted ScalingAnalysis from ScalingLawAnalyzer.analyze()
        visualizer: ScalingVisualizer configured with same output_dir paths
        output_dir: Root directory for all pipeline artifacts
        compute_budgets: Compute budgets used in the sweep
    """

    analysis: ScalingAnalysis
    visualizer: ScalingVisualizer
    output_dir: str | Path = "outputs"
    compute_budgets: list[float] = field(default_factory=list)

    def chinchilla_table(
        self, compute_budgets: list[float] | None = None
    ) -> str:
        """Return Chinchilla-style markdown table of optimal model configurations.

        Args:
            compute_budgets: FLOPs budgets to tabulate. If None, uses 9
                log-spaced budgets from 1e18 to 1e22.

        Returns:
            Markdown-formatted table string with columns:
            Compute Budget | Optimal N | Optimal D | D/N Ratio | Predicted Loss
        """
        return self.analysis.chinchilla_table(compute_budgets)

    def predict(self, compute_budget: float) -> dict:
        """Predict optimal model configuration for a target compute budget.

        Args:
            compute_budget: Target FLOPs

        Returns:
            Dict with keys: target_compute, optimal_params, optimal_tokens,
            expected_loss, tokens_per_param
        """
        return self.analysis.predict_optimal_size(compute_budget)

    def plot(self, show: bool = False) -> list[plt.Figure]:
        """Generate all scaling law visualizations.

        Creates three figures:
        - IsoFLOPs curves (loss vs model size per compute budget)
        - Scaling laws (N_opt, D_opt, L_opt vs compute budget)
        - Tokens-per-param ratio across compute budgets

        Figures are saved as PNGs to {output_dir}/plots/ by the visualizer.

        Args:
            show: If True, display plots interactively via plt.show()

        Returns:
            List of matplotlib Figure objects
        """
        figures = self.visualizer.plot_all(save=True)
        if show:
            plt.show()
        return figures
```

Also update `src/flops_fit/__init__.py` to export Result:
Read `src/flops_fit/__init__.py` first, then add:
```python
from flops_fit.result import Result
```

Run: `uv run pytest tests/test_result.py -v` — all tests MUST pass. Commit: `feat(06-01): implement Result dataclass`
  </action>
  <verify>uv run pytest tests/test_result.py -v 2>&1 | tail -20</verify>
  <done>All tests in test_result.py pass. GREEN phase confirmed. uv run pytest shows no failures in test_result.py.</done>
</task>

</tasks>

<verification>
uv run pytest tests/test_result.py -v
uv run pytest --tb=short -q (all 160+ tests still pass, no regressions)
</verification>

<success_criteria>
- tests/test_result.py created with 9 tests, all passing after implementation
- src/flops_fit/result.py created with Result dataclass
- Result.chinchilla_table() delegates to ScalingAnalysis.chinchilla_table()
- Result.predict() delegates to ScalingAnalysis.predict_optimal_size()
- Result.plot() delegates to ScalingVisualizer.plot_all()
- No existing tests broken (160+ passing)
- Two atomic commits: test(06-01) and feat(06-01)
</success_criteria>

<output>
After completion, create `.planning/phases/06-results-object-and-api-integration/06-01-SUMMARY.md`
</output>
