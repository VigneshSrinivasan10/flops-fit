---
phase: 06-results-object-and-api-integration
plan: 02
type: execute
wave: 2
depends_on:
  - "06-01"
files_modified:
  - src/flops_fit/api.py
  - tests/test_api.py
autonomous: true

must_haves:
  truths:
    - "find_optimal() with train=True + dataset + loss_fn returns a Result object (not list[dict])"
    - "find_optimal() with train=False still returns SweepPlan"
    - "find_optimal() with no dataset still returns SweepPlan"
    - "Result returned from find_optimal() has chinchilla_table(), predict(), plot() methods that work"
    - "results.json is still written to output_dir after training (backward compat)"
  artifacts:
    - path: "src/flops_fit/api.py"
      provides: "Updated find_optimal() returning Result after training"
      contains: "from flops_fit.result import Result"
    - path: "tests/test_api.py"
      provides: "Updated TestFindOptimalTraining expecting Result, resume test adapted"
      contains: "isinstance(result, Result)"
  key_links:
    - from: "src/flops_fit/api.py"
      to: "src/flops_fit/analyzer.py"
      via: "ScalingLawAnalyzer constructed with output_dir-relative paths after training"
      pattern: "ScalingLawAnalyzer.*results_path.*output_path"
    - from: "src/flops_fit/api.py"
      to: "src/flops_fit/visualizer.py"
      via: "ScalingVisualizer constructed with output_dir-relative paths"
      pattern: "ScalingVisualizer.*results_path.*analysis_path.*output_dir"
    - from: "src/flops_fit/api.py"
      to: "src/flops_fit/result.py"
      via: "Result wrapping analysis + visualizer"
      pattern: "Result\\(analysis=analysis.*visualizer=visualizer"
---

<objective>
Update find_optimal() to chain analyze→visualize→return Result after training completes.
Update test_api.py to reflect the new return type (Result instead of list[dict]).

Purpose: This completes the end-to-end pipeline. Users get a single Result object with all three methods instead of a raw list of training dicts.
Output: Updated api.py and test_api.py
</objective>

<execution_context>
@/home/viggie/.claude/get-shit-done/workflows/execute-plan.md
@/home/viggie/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@src/flops_fit/api.py
@tests/test_api.py
@.planning/phases/06-results-object-and-api-integration/06-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update find_optimal() to return Result after training</name>
  <files>src/flops_fit/api.py</files>
  <action>
Read src/flops_fit/api.py first.

In the training branch (where `runner.run_sweep_from_plan(...)` is called), replace the `return runner.run_sweep_from_plan(...)` with a three-step chain:

1. Call `runner.run_sweep_from_plan(...)` but do NOT return its value — the results are written to results.json. Drop the return value (or assign to `_`).

2. After training completes, construct paths relative to output_dir:
```python
from pathlib import Path
from flops_fit.analyzer import ScalingLawAnalyzer
from flops_fit.visualizer import ScalingVisualizer
from flops_fit.result import Result

output_path = Path(output_dir)
analyzer = ScalingLawAnalyzer(
    results_path=output_path / "results.json",
    output_dir=output_path / "analysis",
)
analysis = analyzer.analyze()

visualizer = ScalingVisualizer(
    results_path=output_path / "results.json",
    analysis_path=output_path / "analysis" / "scaling_laws.json",
    output_dir=output_path / "plots",
)
```

3. Wrap and return:
```python
return Result(
    analysis=analysis,
    visualizer=visualizer,
    output_dir=str(output_path),
    compute_budgets=compute_budgets,
)
```

IMPORTANT notes:
- Keep the existing lazy import for TrainingRunner: `from flops_fit.trainer import TrainingRunner` inside the branch (already there, do not move)
- The three new imports (ScalingLawAnalyzer, ScalingVisualizer, Result) should be inside the training branch too, to keep the lazy-import pattern consistent and avoid circular imports
- Do NOT touch the inspection mode path (`return plan`) — SweepPlan return stays unchanged
- Do NOT touch the NotImplementedError path at the bottom
- Update the docstring Returns section to reflect: `Result` when training completes, `SweepPlan` when train=False or dataset/loss_fn omitted

Verify the analysis is done AFTER training: `runner.run_sweep_from_plan(...)` must complete before `analyzer.analyze()` is called.
  </action>
  <verify>uv run python -c "from flops_fit.api import find_optimal; print('import ok')"</verify>
  <done>find_optimal() is importable and the training branch constructs ScalingLawAnalyzer → ScalingVisualizer → Result before returning. No syntax errors.</done>
</task>

<task type="auto">
  <name>Task 2: Update test_api.py TestFindOptimalTraining for Result return type</name>
  <files>tests/test_api.py</files>
  <action>
Read tests/test_api.py first.

The TestFindOptimalTraining class has 5 tests that need updating because find_optimal() no longer returns list[dict]:

**Add import at top of file (with other imports):**
```python
from flops_fit.result import Result
```

**test_find_optimal_executes_training_returns_results:**
Update to verify Result is returned with working methods:
```python
def test_find_optimal_executes_training_returns_result(
    self, tmp_path, tiny_model_cls, tiny_dataset
):
    """find_optimal() returns Result object when dataset+loss_fn provided."""
    result = find_optimal(
        model_cls=tiny_model_cls,
        model_size_param="width",
        dataset=tiny_dataset,
        loss_fn=nn.MSELoss(),
        compute_budgets=[1e8],
        train=True,
        output_dir=str(tmp_path),
    )
    assert isinstance(result, Result)
    # All three API methods must work
    table = result.chinchilla_table()
    assert isinstance(table, str)
    assert "Compute Budget" in table

    pred = result.predict(1e18)
    assert isinstance(pred, dict)
    assert "optimal_params" in pred
    assert pred["expected_loss"] > 0
```

**test_find_optimal_writes_results_json:**
Keep the test body but remove the `first_loss = results1[0]["final_loss"]` pattern — the return value is now a Result. Just call find_optimal() and check the file:
```python
def test_find_optimal_writes_results_json(
    self, tmp_path, tiny_model_cls, tiny_dataset
):
    """find_optimal() writes results.json to output_dir after training."""
    find_optimal(
        model_cls=tiny_model_cls,
        model_size_param="width",
        dataset=tiny_dataset,
        loss_fn=nn.MSELoss(),
        compute_budgets=[1e8],
        train=True,
        output_dir=str(tmp_path),
    )
    results_file = tmp_path / "results.json"
    assert results_file.exists()
    import json
    data = json.loads(results_file.read_text())
    assert isinstance(data, list)
    assert len(data) > 0
```

**test_find_optimal_resume_skips_completed:**
The test currently uses `results1[0]["final_loss"]` to compare losses. Update to compare via Result.predict():
```python
def test_find_optimal_resume_skips_completed(
    self, tmp_path, tiny_model_cls, tiny_dataset
):
    """find_optimal() with resume=True skips already-completed experiments."""
    import json
    # Run once — writes results.json
    find_optimal(
        model_cls=tiny_model_cls,
        model_size_param="width",
        dataset=tiny_dataset,
        loss_fn=nn.MSELoss(),
        compute_budgets=[1e8],
        train=True,
        output_dir=str(tmp_path),
    )
    first_results = json.loads((tmp_path / "results.json").read_text())
    first_loss = first_results[0]["final_loss"]

    # Run again with resume=True
    find_optimal(
        model_cls=tiny_model_cls,
        model_size_param="width",
        dataset=tiny_dataset,
        loss_fn=nn.MSELoss(),
        compute_budgets=[1e8],
        train=True,
        output_dir=str(tmp_path),
        resume=True,
    )
    resumed_results = json.loads((tmp_path / "results.json").read_text())
    # Results file must contain same experiments (not doubled)
    assert resumed_results[0]["final_loss"] == pytest.approx(first_loss)
```

**test_find_optimal_train_false_returns_sweep_plan** and **test_find_optimal_no_dataset_returns_sweep_plan**: Leave unchanged — these verify SweepPlan return, which doesn't change.

Do NOT rename test methods; only update the body and docstring of affected tests. Preserve all other tests in the file unchanged.
  </action>
  <verify>uv run pytest tests/test_api.py -v 2>&1 | tail -30</verify>
  <done>All tests in test_api.py pass. TestFindOptimalTraining tests use Result assertions. SweepPlan tests still pass. No regressions in other test classes.</done>
</task>

</tasks>

<verification>
# Full test suite
uv run pytest --tb=short -q

# Targeted
uv run pytest tests/test_api.py tests/test_result.py -v

# Quick integration smoke test
uv run python -c "
import matplotlib; matplotlib.use('Agg')
import torch, torch.nn as nn
from torch.utils.data import Dataset
from flops_fit import find_optimal
from flops_fit.result import Result
import tempfile, os

class TinyModel(nn.Module):
    def __init__(self, width=8):
        super().__init__()
        self.net = nn.Linear(4, 1)
        self.width = width
    def forward(self, x): return self.net(x)
    def num_params(self): return sum(p.numel() for p in self.parameters())

class TinyDS(Dataset):
    def __init__(self):
        self.x = torch.randn(64, 4)
        self.y = torch.randn(64, 1)
    def __len__(self): return 64
    def __getitem__(self, i): return self.x[i], self.y[i]

with tempfile.TemporaryDirectory() as tmp:
    result = find_optimal(
        model_cls=TinyModel,
        model_size_param='width',
        dataset=TinyDS(),
        loss_fn=nn.MSELoss(),
        compute_budgets=[1e8],
        train=True,
        output_dir=tmp,
    )
    assert isinstance(result, Result), f'Got {type(result)}'
    table = result.chinchilla_table()
    assert 'Compute Budget' in table
    pred = result.predict(1e18)
    assert 'optimal_params' in pred
    print('SMOKE TEST PASSED')
"
</verification>

<success_criteria>
- find_optimal() with train=True + dataset + loss_fn returns Result (not list[dict])
- find_optimal() with train=False still returns SweepPlan
- result.chinchilla_table() returns a markdown table string
- result.predict(1e18) returns a dict with optimal_params, optimal_tokens, expected_loss
- result.plot() returns a list of matplotlib figures
- results.json is still written to output_dir (backward compat, resume still works)
- Full test suite passes (165+ tests expected after new tests from plan 01)
</success_criteria>

<output>
After completion, create `.planning/phases/06-results-object-and-api-integration/06-02-SUMMARY.md`
</output>
