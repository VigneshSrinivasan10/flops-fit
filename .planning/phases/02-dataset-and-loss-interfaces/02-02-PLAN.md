---
phase: 02-dataset-and-loss-interfaces
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - src/flops_fit/api.py
  - tests/test_api.py
autonomous: true

must_haves:
  truths:
    - "find_optimal() validates dataset if provided and raises TypeError for invalid dataset"
    - "find_optimal() validates loss_fn if provided and raises TypeError for invalid loss_fn"
    - "find_optimal() accepts dataset=None and loss_fn=None without error (backward compat with Phase 1)"
    - "find_optimal() validates model, dataset, and loss_fn all at call time before any pipeline work"
    - "Validation order: model first, then dataset, then loss_fn"
  artifacts:
    - path: "src/flops_fit/api.py"
      provides: "find_optimal() with dataset and loss_fn validation"
      exports: ["find_optimal"]
    - path: "tests/test_api.py"
      provides: "API integration tests for dataset/loss validation"
      min_lines: 40
  key_links:
    - from: "src/flops_fit/api.py"
      to: "src/flops_fit/data.py"
      via: "import validate_dataset"
      pattern: "from flops_fit\\.data import validate_dataset"
    - from: "src/flops_fit/api.py"
      to: "src/flops_fit/loss.py"
      via: "import validate_loss_fn"
      pattern: "from flops_fit\\.loss import validate_loss_fn"
    - from: "src/flops_fit/api.py"
      to: "src/flops_fit/model_factory.py"
      via: "import validate_model_contract"
      pattern: "from flops_fit\\.model_factory import validate_model_contract"
---

<objective>
Wire dataset and loss validation into find_optimal() so that users get clear errors at call time when they pass invalid dataset or loss_fn arguments.

Purpose: This completes the Phase 2 goal -- users can pass dataset and loss_fn to find_optimal() and the library validates both interfaces immediately. Invalid inputs fail fast with actionable error messages instead of crashing deep in a training loop.
Output: Updated api.py with validation calls, test_api.py with integration tests.
</objective>

<execution_context>
@/home/viggie/.claude/get-shit-done/workflows/execute-plan.md
@/home/viggie/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-dataset-and-loss-interfaces/02-RESEARCH.md
@.planning/phases/02-dataset-and-loss-interfaces/02-01-SUMMARY.md
@src/flops_fit/api.py
@src/flops_fit/__init__.py
@.planning/codebase/CONVENTIONS.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Integrate dataset and loss validation into find_optimal()</name>
  <files>
    src/flops_fit/api.py
  </files>
  <action>
Update `src/flops_fit/api.py` to add dataset and loss validation to `find_optimal()`.

Add imports at top:
```python
from flops_fit.data import validate_dataset
from flops_fit.loss import validate_loss_fn
```

In `find_optimal()`, AFTER the existing `validate_model_contract()` call, add:

```python
# Validate dataset interface (Phase 2)
if dataset is not None:
    validate_dataset(dataset)

# Validate loss function (Phase 2)
if loss_fn is not None:
    validate_loss_fn(loss_fn)
```

Keep dataset and loss_fn as optional (None default). Validation only runs if the user actually provides them. This preserves backward compatibility -- calling find_optimal() with just model args still works exactly as before.

The validation order must be: model -> dataset -> loss_fn. This way the user fixes the most fundamental issue first.

Do NOT change the function signature -- dataset=None and loss_fn=None should already be there from Phase 1's library skeleton plan. If for any reason the signature doesn't include these params, add them.

Do NOT change the NotImplementedError at the end -- the full pipeline isn't built until Phase 6.
  </action>
  <verify>
`python -c "from flops_fit.api import find_optimal; print('OK')"` -- importable.
`uv run ruff check src/flops_fit/api.py` -- clean.
  </verify>
  <done>
- find_optimal() calls validate_dataset when dataset is not None
- find_optimal() calls validate_loss_fn when loss_fn is not None
- Validation order: model, dataset, loss_fn
- Existing behavior preserved for dataset=None, loss_fn=None
  </done>
</task>

<task type="auto">
  <name>Task 2: Add API integration tests for dataset/loss validation</name>
  <files>
    tests/test_api.py
  </files>
  <action>
Update `tests/test_api.py` (which should exist from Phase 1's library skeleton plan) to add integration tests for dataset and loss validation through find_optimal().

Use the same MockModel class that Phase 1's test_api.py defines (a simple class with num_params()).

Add a minimal torch Dataset fixture for testing:
```python
class TinyDataset(torch.utils.data.Dataset):
    def __init__(self):
        self.data = torch.randn(10, 4)
    def __len__(self):
        return 10
    def __getitem__(self, idx):
        return self.data[idx]
```

Add test class `TestFindOptimalDatasetValidation`:
1. `test_accepts_valid_dataset` -- find_optimal with MockModel + TinyDataset + valid loss_fn. Raises NotImplementedError (validation passed).
2. `test_accepts_none_dataset` -- find_optimal with MockModel, dataset=None, loss_fn=None. Raises NotImplementedError (backward compat).
3. `test_rejects_invalid_dataset` -- find_optimal with MockModel + dataset=[1,2,3]. Raises TypeError (not NotImplementedError).
4. `test_model_validated_before_dataset` -- find_optimal with bad model class AND bad dataset. Raises TypeError about the MODEL (not about dataset), proving model is validated first.

Add test class `TestFindOptimalLossValidation`:
1. `test_accepts_valid_loss` -- find_optimal with MockModel + TinyDataset + `torch.nn.MSELoss()`. Raises NotImplementedError.
2. `test_accepts_none_loss` -- find_optimal with MockModel + TinyDataset + loss_fn=None. Raises NotImplementedError.
3. `test_rejects_non_callable_loss` -- find_optimal with MockModel + loss_fn=42. Raises TypeError.
4. `test_dataset_validated_before_loss` -- find_optimal with MockModel + bad dataset + bad loss_fn. Raises TypeError about DATASET (not about loss_fn), proving order is model -> dataset -> loss_fn.

Use `pytest.raises(TypeError)` and `pytest.raises(NotImplementedError)` as appropriate. Check error message content with `match=` parameter where it adds clarity.
  </action>
  <verify>
`uv run pytest tests/test_api.py -v` -- all tests pass (including Phase 1's existing tests).
`uv run ruff check tests/test_api.py` -- clean.
  </verify>
  <done>
- Integration tests prove find_optimal validates dataset and loss_fn
- Backward compatibility confirmed: None values still work
- Validation order tested: model -> dataset -> loss_fn
- All tests pass including pre-existing ones
  </done>
</task>

</tasks>

<verification>
1. `uv run pytest tests/test_api.py tests/test_data.py tests/test_loss.py -v` -- all tests pass
2. `uv run pytest tests/ -v` -- full suite passes (no regressions)
3. `uv run ruff check src/flops_fit/ tests/` -- clean
4. Manual verification: `python -c "import flops_fit; flops_fit.find_optimal(model_cls=int, model_size_param='x')"` raises TypeError about model contract (not about dataset/loss)
</verification>

<success_criteria>
- find_optimal() validates dataset and loss_fn at call time with clear errors
- None values for dataset/loss_fn are accepted (backward compat)
- Validation order is model -> dataset -> loss_fn
- Full test suite passes with no regressions
- Phase 2 success criteria met: users can pass dataset/loss objects and get immediate validation
</success_criteria>

<output>
After completion, create `.planning/phases/02-dataset-and-loss-interfaces/02-02-SUMMARY.md`
</output>
