---
phase: 02-dataset-and-loss-interfaces
plan: 01
type: tdd
wave: 1
depends_on: []
files_modified:
  - src/flops_fit/data.py
  - src/flops_fit/loss.py
  - tests/test_data.py
  - tests/test_loss.py
autonomous: true

must_haves:
  truths:
    - "validate_dataset accepts torch Dataset and returns without error"
    - "validate_dataset accepts torch DataLoader and returns without error"
    - "validate_dataset accepts IterableDataset and returns without error"
    - "validate_dataset rejects non-Dataset objects with TypeError mentioning HuggingFace hint"
    - "wrap_dataset returns a DataLoader when given a Dataset"
    - "wrap_dataset passes through an existing DataLoader unchanged"
    - "wrap_dataset rejects non-Dataset/DataLoader with TypeError"
    - "validate_loss_fn accepts callable with 2+ positional args"
    - "validate_loss_fn accepts nn.Module instances with forward(self, pred, target)"
    - "validate_loss_fn rejects non-callable with TypeError"
    - "validate_loss_fn warns when loss_fn is a class (not instance) that is an nn.Module subclass"
    - "validate_loss_fn gracefully handles C-extension callables where inspect.signature fails"
  artifacts:
    - path: "src/flops_fit/data.py"
      provides: "Dataset validation and DataLoader wrapping"
      exports: ["validate_dataset", "wrap_dataset"]
    - path: "src/flops_fit/loss.py"
      provides: "Loss function validation"
      exports: ["validate_loss_fn"]
    - path: "tests/test_data.py"
      provides: "Tests for data validation and wrapping"
      min_lines: 80
    - path: "tests/test_loss.py"
      provides: "Tests for loss function validation"
      min_lines: 60
  key_links:
    - from: "src/flops_fit/data.py"
      to: "torch.utils.data"
      via: "isinstance checks against Dataset, DataLoader, IterableDataset"
      pattern: "isinstance.*data\\.(Dataset|DataLoader|IterableDataset)"
    - from: "src/flops_fit/loss.py"
      to: "inspect"
      via: "signature inspection for parameter count"
      pattern: "inspect\\.signature"
---

<objective>
Create the dataset validation/wrapping module (data.py) and loss function validation module (loss.py) with comprehensive tests using TDD.

Purpose: These are the core validation modules that Phase 2's API integration (Plan 02) will wire into find_optimal(). They validate user-provided dataset and loss_fn inputs at call time with clear error messages, before any training begins.
Output: data.py, loss.py, test_data.py, test_loss.py -- all tests green.
</objective>

<execution_context>
@/home/viggie/.claude/get-shit-done/workflows/execute-plan.md
@/home/viggie/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-dataset-and-loss-interfaces/02-RESEARCH.md
@.planning/codebase/CONVENTIONS.md
@.planning/codebase/TESTING.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: TDD data.py -- dataset validation and wrapping</name>
  <files>
    tests/test_data.py
    src/flops_fit/data.py
  </files>
  <action>
**RED phase:** Create `tests/test_data.py` with test classes and cases FIRST. Use real torch types for test fixtures.

Define a minimal map-style Dataset for testing:
```python
class SimpleDataset(torch.utils.data.Dataset):
    def __init__(self, size=100):
        self.data = torch.randn(size, 10)
    def __len__(self):
        return len(self.data)
    def __getitem__(self, idx):
        return self.data[idx]
```

Define a minimal IterableDataset:
```python
class SimpleIterableDataset(torch.utils.data.IterableDataset):
    def __iter__(self):
        for i in range(100):
            yield torch.randn(10)
```

Test class `TestValidateDataset`:
1. `test_accepts_map_dataset` -- pass SimpleDataset, no error raised
2. `test_accepts_dataloader` -- pass DataLoader(SimpleDataset()), no error raised
3. `test_accepts_iterable_dataset` -- pass SimpleIterableDataset, no error raised
4. `test_rejects_plain_list` -- pass `[1, 2, 3]`, raises TypeError
5. `test_rejects_dict` -- pass `{"a": 1}`, raises TypeError
6. `test_rejects_none` -- pass `None`, raises TypeError
7. `test_error_message_mentions_huggingface` -- error message for rejected type contains "with_format" hint
8. `test_error_message_includes_type_name` -- error for `[1,2,3]` includes "list" in message

Test class `TestWrapDataset`:
1. `test_wraps_dataset_to_dataloader` -- pass SimpleDataset, returns DataLoader instance
2. `test_wrapped_dataloader_is_iterable` -- iterate one batch from wrapped DataLoader, get a tensor
3. `test_passthrough_existing_dataloader` -- pass DataLoader(SimpleDataset()), returns the SAME DataLoader object (identity check with `is`)
4. `test_respects_batch_size` -- wrap with batch_size=16, iterate one batch, check batch dimension is 16
5. `test_rejects_non_dataset` -- pass a string, raises TypeError
6. `test_wraps_iterable_dataset` -- pass SimpleIterableDataset, returns DataLoader (shuffle must be False for IterableDataset)

Run tests -- they must ALL FAIL (module doesn't exist yet).

**GREEN phase:** Create `src/flops_fit/data.py` with:

Module docstring describing purpose. Module-level logger per conventions.

`validate_dataset(dataset_or_loader)`:
- If `isinstance(dataset_or_loader, data.DataLoader)`: return (valid)
- If `isinstance(dataset_or_loader, data.IterableDataset)`: return (valid, no __len__ check)
- If `isinstance(dataset_or_loader, data.Dataset)`: return (valid)
- Otherwise: raise TypeError with message including type name, "torch.utils.data.Dataset or torch.utils.data.DataLoader", and HuggingFace hint: "If using HuggingFace datasets, call `dataset.with_format('torch')` first."
- If None: raise TypeError with message "dataset is required" (explicit None check before isinstance)

`wrap_dataset(dataset_or_loader, batch_size=32, num_workers=0, shuffle=True)`:
- If DataLoader: return as-is (passthrough)
- If IterableDataset: wrap in DataLoader with shuffle=False (IterableDataset does not support shuffle), drop_last=True, given batch_size and num_workers
- If Dataset: wrap in DataLoader with given batch_size, shuffle, num_workers, drop_last=True
- Otherwise: raise TypeError (same message pattern as validate_dataset)

Type annotations on all functions per conventions. Return types annotated.

Run tests -- they must ALL PASS.

**REFACTOR:** Clean up if needed. Ensure ruff passes.
  </action>
  <verify>
`uv run pytest tests/test_data.py -v` -- all tests pass.
`uv run ruff check src/flops_fit/data.py tests/test_data.py` -- no lint errors.
  </verify>
  <done>
- validate_dataset accepts Dataset, DataLoader, IterableDataset; rejects everything else with helpful TypeError
- wrap_dataset normalizes Dataset/IterableDataset to DataLoader; passes through existing DataLoader
- Error messages include type name and HuggingFace hint
- All tests green, ruff clean
  </done>
</task>

<task type="auto">
  <name>Task 2: TDD loss.py -- loss function validation</name>
  <files>
    tests/test_loss.py
    src/flops_fit/loss.py
  </files>
  <action>
**RED phase:** Create `tests/test_loss.py` with test cases FIRST.

Test class `TestValidateLossFn`:
1. `test_accepts_plain_function` -- pass `lambda pred, target: (pred - target).mean()`, no error
2. `test_accepts_nn_module_instance` -- pass `torch.nn.CrossEntropyLoss()`, no error
3. `test_accepts_custom_nn_module` -- define a Module with `forward(self, pred, target)`, pass instance, no error
4. `test_accepts_function_with_extra_kwargs` -- pass `def my_loss(pred, target, weight=1.0): ...`, no error (has 2+ positional)
5. `test_accepts_var_args` -- pass `def my_loss(*args): ...`, no error (has *args)
6. `test_rejects_none` -- pass None, raises TypeError with "loss_fn is required" in message
7. `test_rejects_non_callable` -- pass `42`, raises TypeError with "callable" in message
8. `test_rejects_zero_arg_callable` -- pass `lambda: 0`, raises TypeError with "at least 2 positional" in message
9. `test_rejects_one_arg_callable` -- pass `lambda x: x`, raises TypeError
10. `test_warns_class_not_instance` -- pass `torch.nn.CrossEntropyLoss` (the CLASS, no parentheses). Raises TypeError with "Did you mean" and "with parentheses" in message. Use `inspect.isclass` detection.
11. `test_handles_uninspectable_callable` -- create a callable where `inspect.signature` would fail (e.g., a C builtin or use `unittest.mock.MagicMock()`). Should NOT raise -- gracefully skip signature check.

Test class `TestGetName`:
1. `test_function_name` -- `_get_name(lambda: 0)` returns `"<lambda>"`
2. `test_module_name` -- `_get_name(torch.nn.CrossEntropyLoss())` returns `"CrossEntropyLoss"`

Run tests -- they must ALL FAIL.

**GREEN phase:** Create `src/flops_fit/loss.py` with:

Module docstring. Module-level logger.

`validate_loss_fn(loss_fn)`:
1. If `loss_fn is None`: raise TypeError with helpful message including examples (CrossEntropyLoss(), lambda, custom callable).
2. If `inspect.isclass(loss_fn)` and `issubclass(loss_fn, torch.nn.Module)`: raise TypeError with "loss_fn appears to be a class, not an instance. Did you mean `loss_fn={class_name}()` (with parentheses)?"
3. If `not callable(loss_fn)`: raise TypeError with "loss_fn must be callable, got {type name}"
4. Try `inspect.signature(loss_fn)` (or for nn.Module instances, inspect `loss_fn.forward`):
   - Count positional params (POSITIONAL_ONLY + POSITIONAL_OR_KEYWORD), excluding `self`
   - Check for VAR_POSITIONAL (*args)
   - If < 2 positional and no *args: raise TypeError with "must accept at least 2 positional arguments (predictions, targets)"
5. Wrap inspect.signature in try/except (ValueError, TypeError) -- if signature inspection fails, skip check silently (C extensions, some builtins).

For nn.Module instances, inspect `loss_fn.forward` instead of `loss_fn.__call__` (since __call__ has *args, **kwargs wrapping). This is critical for getting accurate parameter counts on nn.Module subclasses.

`_get_name(loss_fn)`:
- If `hasattr(loss_fn, '__name__')`: return `loss_fn.__name__`
- Else: return `type(loss_fn).__name__`

Type annotations. Return types.

Run tests -- they must ALL PASS.

**REFACTOR:** Clean up. Ensure ruff passes.
  </action>
  <verify>
`uv run pytest tests/test_loss.py -v` -- all tests pass.
`uv run ruff check src/flops_fit/loss.py tests/test_loss.py` -- no lint errors.
  </verify>
  <done>
- validate_loss_fn accepts functions, nn.Module instances, callables with *args
- Rejects None (with examples), non-callable (with type name), wrong arity (with expected signature)
- Detects class-vs-instance mistake for nn.Module subclasses
- Gracefully handles uninspectable callables (C extensions)
- All tests green, ruff clean
  </done>
</task>

</tasks>

<verification>
1. `uv run pytest tests/test_data.py tests/test_loss.py -v` -- all tests pass
2. `uv run ruff check src/flops_fit/data.py src/flops_fit/loss.py` -- clean
3. `python -c "from flops_fit.data import validate_dataset, wrap_dataset; print('OK')"` -- importable
4. `python -c "from flops_fit.loss import validate_loss_fn; print('OK')"` -- importable
</verification>

<success_criteria>
- data.py validates Dataset, DataLoader, IterableDataset; rejects invalid inputs with actionable errors
- loss.py validates callables with 2+ positional args; detects class-vs-instance mistake; handles C extensions
- Error messages are specific: include type name, expected interface, HuggingFace hint (for data), examples (for loss)
- All TDD tests pass, ruff clean
</success_criteria>

<output>
After completion, create `.planning/phases/02-dataset-and-loss-interfaces/02-01-SUMMARY.md`
</output>
